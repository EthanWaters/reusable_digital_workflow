### Initialize Libraries -------------------------------------------------------------

# Include functions from source code R file. 
source("source.R")

# Include libraries to perform this workflow. 
library("tools")
library("installr")
library("sets")
library("methods")
library("rio")
library("dplyr")
library("stringr")
library("fastmatch")
library("lubridate")
library("rlang")
library("purrr")
library("jsonlite")
library("sf")
library("raster")
library("terra")
library("units")
library("tidyverse")
library("tidyr")
library("lwgeom")
library("stars")
library("stringr")
library("future")
library("furrr")
library("foreach")
library("doParallel")
library("digest")

# Process GBRMPA control data into a standardised format that researchers have utilised
# historically. Verification functions were written in consultation with domain experts 
# determine whether a row contains an error and is suitable for use in research. 
# The ecological observations are assigned to the nearest site.
main <- function(new_path, configuration_path = NULL, kml_path = NULL, leg_path = NULL) {
## `new_path` = Path to the file containing the new control data. This new file should 
# contain the entire historical dataset exported from GBRMPA EOTR database
## `configuration_path` = Path to JSON configuration file utilised to specify desired 
# setting. This has a specific format and should remain consistent with historically 
# devloped config files. 
## `kml_path` = Path to kml spatial file containing the GBRMPA cull sites. Polygons 
# within this file are the basis for determining the nearest site. 
## `leg_path` = Path to legacy control data. This is a CSV contain data previously 
# processed by this workflow. It is used as a point of comparison and is optional. 

  
  tryCatch({

    ### Initialize Required Variables -------------------------------------------------------------
    # Get the keyword from the new input file used to identify what type of control 
    # data it is. 
    keyword <- get_file_keyword(new_path) 

    # When a configuration file is not specified specifically with a path then the 
    # most recent config file with the keyword is utilsied 
    if (is.null(configuration_path)) {
      configuration_path <- find_recent_file("configuration_files/", paste("research_",keyword, sep=""), "json")
      configuration <- fromJSON(configuration_path)
    }

    # If parameters are not provided the workflow checks the expected location for 
    # the most recent file that meets the requirements. 
    most_recent_kml_path <- find_recent_file(configuration$metadata$input_directory$spatial_data, "Sites", "kml")
    most_recent_leg_path <- find_recent_file(configuration$metadata$output_directory$control_data_unaggregated, configuration$metadata$control_data_type, "csv")

    # The workflow accesses the report and serialised raster data generated by the 
    # previous instance of this workflow. This information can be utilised later to
    # reduce the number of spatial operations.
    most_recent_report_path <- find_recent_file(configuration$metadata$output_directory$reports, configuration$metadata$control_data_type, "json")
    serialised_spatial_path <- find_recent_file(configuration$metadata$output_directory$spatial_data, "site_regions", "rds")
    separate_data <- configuration$metadata$separate_data

    
    previous_kml_path <- NULL
    if(!is.null(most_recent_report_path)){
      previous_report <- fromJSON(most_recent_report_path)
      previous_kml_path <- previous_report$inputs$kml_path
    } 

    # Attempt to use legacy data where possible. 
    # The workflow "is_new" if there is no evidence that the workflow has be run 
    # previously. 
    is_new <- 0
    is_legacy_data_available <- 1
    if (is.null(leg_path)) {
      leg_path <- most_recent_leg_path
      if(is.null(most_recent_leg_path)){
        is_new <- 1
        is_legacy_data_available <- 0
      } else {
        leg_path <- most_recent_leg_path
      }
    } 
    
    if (is.null(kml_path)) {
      kml_path <- most_recent_kml_path
    }

    ### Import Data -------------------------------------------------------------
    # import new and legacy data. 
    # A legacy dataset with no error_flag column indicates it is new becuase R implementation 
    # of this workflow exports an error flag column, whereas the mathmatica implementation 
    # did not
    new_data_df <- rio::import(new_path)
    if(is_legacy_data_available){
      legacy_df <- rio::import(leg_path)
      if("error_flag" %in% colnames(legacy_df)){
        is_new <- 0
      } else {
        is_new <- 1
        legacy_df["error_flag"] <- 0
      }
    }
    
    # Check if the new data has an authoritative ID. All rows of a database export 
    # will have one and no rows from a powerBI export will. There should be no 
    # scenario where only a portion of rows have IDs. Even if an ID is present it 
    # should be ensured that it is authoritative before altering the configuration 
    # files to preference the use of the ID for separation rather than checking 
    # for differences manually. 
    
    has_authorative_ID <-  !any(is.na(new_data_df[[configuration$metadata$ID_col]])) & configuration$metadata$is_ID_preferred
    assign("has_authorative_ID", has_authorative_ID, envir = .GlobalEnv) 


    ### Setup Report -------------------------------------------------------------
    
    # create list to export as json file with metadata about workflow
    metadata_json_output <- list()    
    
    # timestamp workflow 
    metadata_json_output[["timestamp"]] = Sys.time()
    
    # record inputs to workflow
    metadata_json_output[["inputs"]] = list(
      kml_path = kml_path,
      new_path = new_path,
      leg_path = leg_path, 
      configuration_path = configuration_path,
      serialised_spatial_path = serialised_spatial_path
    )
    
    # record inputs to workflow
    metadata_json_output[["decisions"]] = list(
      has_authorative_ID = has_authorative_ID,
      is_legacy_data_available = is_legacy_data_available,
      is_new = is_new
    )
    
    # save metadata json file 
    json_data <- toJSON(metadata_json_output, pretty = TRUE)
    writeLines(json_data, file.path(getwd(), configuration$metadata$output_directory$reports, paste(configuration$metadata$control_data_type, "_Report_", format(Sys.time(), "%Y%m%d_%H%M%S"), ".json", sep = "")))
    

    ### Transform Data -------------------------------------------------------------
    # Transform the new incoming data into the desired structure. Create new 
    # columns that are required but not present in the incoming dataset and 
    # populate them with defaul values that can be constant or derived from 
    # other columns in the dataset.
    transformed_data_df <- map_data_structure(new_data_df, configuration$mappings$transformations, configuration$mappings$new_fields)

    # Set the data type of both datasets to ensure that any operations or 
    # comparisons are executed in the expected manner. Rows that fail to parse 
    # indicate a fundamental flaw in the dataset and should be addressed by the
    # providers of the dataset.
    if(is_legacy_data_available){
      legacy_df <- set_data_type(legacy_df, configuration$mappings$data_type_mappings)
    }
    formatted_data_df <- set_data_type(transformed_data_df, configuration$mappings$data_type_mappings) 

    # This function can help automate generating new config file versions
    # adjusting to changes in the expected input. Currently has minor bugs 
    # and is optional but helpful
    # configuration <- update_config_file(new_data_df, configuration_path)


    ### Verify Data -------------------------------------------------------------
    # verification functions flag whether a row is deemed to have an error. 
    # Although there may be situations where subsets of these rows can still 
    # be utilised it provides a quick way to filter out data not considered 
    # ideal for research purposes. This puts the onus on the researcher to 
    # justify their inclusion of error flagged data.
    verified_data_df <- verify_entries(formatted_data_df, configuration)
    if(is_new && is_legacy_data_available){
      legacy_df <- verify_entries(legacy_df, configuration) 
    }
    
    # flag non-genuine duplicates that are mistakes. There are possible 
    # instances where two perfect matches are not duplicates but rather two 
    # identical valid data points (predominantly seen in cull data). 
    # Typically errorous data will be 3 or more duplications and as a result, 
    # this is the only situation where duplicates are flagged as errors. 
    verified_data_df <- flag_duplicates(verified_data_df)

    ### Assign Sites to Data -------------------------------------------------------------
    # This process determines the site number where manta tow and culls were performed.
    # Although cull sites have the site name included in the dataset, including the 
    # site number makes it easier for researchers to compare the datasets.
    tryCatch({
      if(configuration$metadata$assign_sites){
        if(configuration$metadata$control_data_type == "manta_tow"){

          # Assigns sites to manta tows with method similar to intial mathmatica 
          # implementation.
          verified_data_df <- assign_nearest_site_method_c(verified_data_df, kml_path, configuration$metadata$control_data_type, previous_kml_path, serialised_spatial_path, configuration$metadata$output_directory$spatial_data, raster_size=0.0005, x_closest=1, is_standardised=0, save_spatial_as_raster=0)
        } else {
          verified_data_df$`Nearest Site` <- site_names_to_numbers(verified_data_df$`Site Name`)
        }
        verified_data_df$`Nearest Site` <- ifelse(is.na(verified_data_df$`Nearest Site`), -1, verified_data_df$`Nearest Site`)
      }
    }, error = function(e) {
      print(paste("Error assigning sites:", conditionMessage(e)))
    })


    ### Separate Data -------------------------------------------------------------
    # Datasets are provided as a single large tabular file which contain all control 
    # data from the inception of the COTS control program to present date. Consequently, 
    # a large portion of the rows will have been processed previously and will be seen 
    # in the legacy dataset. This provides the oppirtunity to seperate the new dataset 
    # into sections to perform further error checking. 

    # The three sections that the data from "new_path" can be split into are, new, perfect
    # match and discrepancy. New is data that this workflow has never seen before. perfect 
    # match is data that is present in the legacy data set and has therefore been processed 
    # previously by this workflow. Discrepancy is a row deemed to have been processed
    # previously but has has minor changes since which could be either quality assurance or 
    # a mistake. 

    # Separating the data can provide the oppirtunity to identify and prevent discrepancies  
    # that transition from having no flagged error in the legacy dataset to a flagged error 
    # as this is likely a mistake. This process can be time consuming and is up to the 
    # user to determine if the trade off is beneficial for their applications. 
    tryCatch({
      if(is_legacy_data_available & separate_data){
        verified_data_df <- separate_control_dataframe(verified_data_df, legacy_df, has_authorative_ID)
      }
    }, error = function(e) {
      print(paste("Error seperating control data. All data has been treated as new entries.", conditionMessage(e)))
    })


    ### Save & Aggregate Data -------------------------------------------------------------
    
    # Save unaggrgated workflow with specific naming convension output
    tryCatch({
      if (!dir.exists(configuration$metadata$output_directory$control_data_unaggregated)) {
        dir.create(configuration$metadata$output_directory$control_data_unaggregated, recursive = TRUE)
      }
      
      output_directory <- configuration$metadata$output_directory$control_data_unaggregated
      data_type <- configuration$metadata$control_data_type
      timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
      file_name <- paste(data_type, "_", timestamp, ".csv", sep = "")
      output_path <- file.path(output_directory, file_name)
      write.csv(verified_data_df, output_path, row.names = FALSE)
      
    }, error = function(e) {
      print(paste("Error saving data - Data saved in source directory", conditionMessage(e)))
      write.csv(verified_data_df, paste(configuration$metadata$control_data_type,"_", format(Sys.time(), "%Y%m%d_%H%M%S"), ".csv", sep = ""), row.names = FALSE)
  
    })

    # Aggregate the cull an manta tow data 
    if(configuration$metadata$control_data_type == "manta_tow"){
      verified_aggregated_df <- aggregate_manta_tows_site_resolution_research(verified_data_df)  
    } else if (configuration$metadata$control_data_type == "cull") {
      verified_aggregated_df <- aggregate_culls_site_resolution_research(verified_data_df) 
    }

    # Save aggregated data
    tryCatch({
      if (!dir.exists(configuration$metadata$output_directory$control_data_aggregated)) {
        dir.create(configuration$metadata$output_directory$control_data_aggregated, recursive = TRUE)
      }
      
      output_directory <- configuration$metadata$output_directory$control_data_aggregated
      data_type <- configuration$metadata$control_data_type
      timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
      file_name <- paste(data_type, "_", timestamp, ".csv", sep = "")
      output_path <- file.path(output_directory, file_name)
      write.csv(verified_aggregated_df, output_path, row.names = FALSE)
      
    }, error = function(e) {
      print(paste("Error saving data - Data saved in source directory", conditionMessage(e)))
      write.csv(verified_aggregated_df, paste(configuration$metadata$control_data_type,"_", format(Sys.time(), "%Y%m%d_%H%M%S"), ".csv", sep = ""), row.names = FALSE)
      
    })
    
    # Reset
    new_path <- NULL
    configuration_path <- NULL
    kml_path <- NULL
    leg_path <- NULL
    
  }, error = function(e) {
    print(paste("Critical Error in workflow could not be resolved:", conditionMessage(e)))
  })
}


### Command Line -----------------------------
# The following allows the code to be executed from the command line provided the syntax is followed.
# All parameters are follow this syntax:
# {NEW PATH} --config={CONFIG PATH} --kml={KML PATH} --leg={LEG PATH}


args <- commandArgs(trailingOnly = TRUE)
new_path <- args[1]

# Initialize optional arguments as NULL
configuration_path <- NULL
kml_path <- NULL
leg_path <- NULL

# Loop through the arguments to find optional ones
for (i in 2:length(args)) {
  if (startsWith(args[i], "--config=")) {
    new_path <- sub("--config=", "", args[i])
  } else if (startsWith(args[i], "--kml=")) {
    kml_path <- sub("--kml=", "", args[i])
  } else if (startsWith(args[i], "--leg=")) {
    leg_path <- sub("--leg=", "", args[i])
  }
}

main(configuration_path, aggregate , new_path, kml_path, leg_path)



