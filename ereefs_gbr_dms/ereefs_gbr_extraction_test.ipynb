{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README: Spatial Data Filtering from RIMReP DMS\n",
    "\n",
    "This project extends functionalities from the RIMReP-Examples repository to enable spatial data filtering based on specified polygons outlined in a shapefile.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The RIMReP DMS repository provides example notebooks in both R and Python, showcasing methods to access datasets available within the Reef 2050 Integrated Monitoring and Reporting Program Data Management System (RIMReP DMS). This project enhances the capabilities by facilitating data extraction with polygon filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_path = \"s3://gbr-dms-data-public/aims-ereefs-biogeochem-baseline-monthly/data.zarr\"\n",
    "shapefile_path = \"D:\\COTS\\COTS_tow_prediction_interface\\gbr\\TS_AIMS_NESP_Torres_Strait_Features_V1b_with_GBR_Features.shp\"\n",
    "variables = ['TOTAL_NITROGEN', 'Chl_a_sum', 'PhyL_Chl', 'PhyS_Chl', 'Oxy_sat', 'MA_N_pr', 'Secchi', 'Kd_490', 'DOR_P', 'DOR_N', 'DOR_C', 'salt', 'temp', 'TN', 'TC', 'TP']\n",
    "date1 = '2010-11-30'\n",
    "date2 = '2019-03-01'\n",
    "depths = [-1,-2]\n",
    "filename = \"ereerfs_aggregations_biogeochemistry_monthly_shortlist_variables_OCT_MAR.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "import geopandas as gpd\n",
    "from rasterio import features\n",
    "from affine import Affine\n",
    "import numpy as np\n",
    "\n",
    "def transform_from_latlon(lat, lon):\n",
    "    lat = np.asarray(lat)\n",
    "    lon = np.asarray(lon)\n",
    "    trans = Affine.translation(lon[0], lat[0])\n",
    "    scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "    return trans * scale\n",
    "\n",
    "def rasterize(shapes, coords, latitude='latitude', longitude='longitude', data_type=float, fill=np.nan, **kwargs):\n",
    "  \n",
    "    transform = transform_from_latlon(coords[latitude], coords[longitude])\n",
    "    out_shape = (len(coords[latitude]), len(coords[longitude]))\n",
    "    raster = features.rasterize(shapes, out_shape=out_shape,\n",
    "                                fill=fill, transform=transform,\n",
    "                                dtype=data_type, **kwargs)\n",
    "    spatial_coords = {latitude: coords[latitude], longitude: coords[longitude]}\n",
    "    return xr.DataArray(raster, coords=spatial_coords, dims=(latitude, longitude))\n",
    "\n",
    "def add_shape_coord_from_data_array(xr_da, shp_path, coord_name):\n",
    "    # 1. read in shapefile\n",
    "    shp_gpd = gpd.read_file(shp_path)\n",
    "\n",
    "    # 2. create a list of tuples (shapely.geometry, id)\n",
    "    #    this allows for many different polygons within a .shp file (e.g. States of US)\n",
    "    shapes = [(shape, n) for n, shape in enumerate(shp_gpd.geometry)]\n",
    "    \n",
    "    xr_da[coord_name] = rasterize(shapes, xr_da.coords, longitude='longitude', latitude='latitude', data_type = float)\n",
    "\n",
    "    return xr_da\n",
    "\n",
    "def ids_to_labels(reef_ids, shp_path, chunk_size=10000):\n",
    "    shp_gpd = gpd.read_file(shp_path)\n",
    "\n",
    "    reef_labels = {reef_label: n for n, reef_label in enumerate(shp_gpd.LOC_NAME_S)}\n",
    "\n",
    "    # Determine the total number of chunks needed\n",
    "    total_chunks = len(reef_ids) // chunk_size + 1\n",
    "    output_labels = reef_ids.copy()\n",
    "    output_labels = output_labels.astype(str)\n",
    "    # Iterate over chunks\n",
    "    for chunk_num in range(total_chunks):\n",
    "        start_idx = chunk_num * chunk_size\n",
    "        end_idx = min((chunk_num + 1) * chunk_size, len(reef_ids))\n",
    "       \n",
    "        xr_chunk = reef_ids[slice(start_idx, end_idx)] \n",
    "        for label, id_val in reef_labels.items():\n",
    "            \n",
    "            xr_chunk = xr_chunk.where(xr_chunk != id_val, other=label)\n",
    "\n",
    "        output_labels[slice(start_idx, end_idx)] = xr_chunk\n",
    "        \n",
    "    return output_labels\n",
    "    \n",
    "    \n",
    "def remove_na(data, chunk_size=10000):\n",
    "    \n",
    "    # Determine the total number of chunks needed\n",
    "    total_chunks = len(data) // chunk_size + 1\n",
    "    output_data = data.copy()\n",
    "    output_data = output_data.astype(str)\n",
    "   \n",
    "    # Iterate over chunks\n",
    "    for chunk_num in range(total_chunks):\n",
    "        start_idx = chunk_num * chunk_size\n",
    "        end_idx = min((chunk_num + 1) * chunk_size, len(data))\n",
    "       \n",
    "        xr_chunk = data[slice(start_idx, end_idx)] \n",
    "        xr_chunk.where(xr_chunk.reef_id.notnull(), drop=True)\n",
    "        \n",
    "        output_data[slice(start_idx, end_idx)] = xr_chunk\n",
    "        \n",
    "    return output_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "data = xr.open_dataset(s3fs.S3Map(zarr_path, s3=fs), engine=\"zarr\")\n",
    "data_output = data.sel(time=slice(date1, date2), k=depths)\n",
    "data_output = data_output[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output_shapefile = add_shape_coord_from_data_array(data_output, shapefile_path, \"reef_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output_shapefile_filtered = remove_na(data_output_shapefile)\n",
    "reef_ids = data_output_shapefile_filtered.reef_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = ids_to_labels(reef_ids, shapefile_path, chunk_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output_shapefile_filtered[\"reef_label\"] = data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output_shapefile_filtered.to_dataframe().to_csv(\"output\\\\\" + filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac4bbee1919ea7aef9c6cfd22a25c3c68d6ebab6b5f3e31fb17a356d6f400124"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
