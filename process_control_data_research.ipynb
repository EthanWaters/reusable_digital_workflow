{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Resusable Digital Work flow\r\n",
        "Process GBRMPA control data into a standardised format that researchers have utilised historically. Verification functions were written in consultation with domain experts determine whether a row contains an error and is suitable for use in research. The ecological observations are assigned to the nearest site."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%r\r\n",
        "library(\"tools\")\r\n",
        "library(\"installr\")\r\n",
        "library(\"sets\")\r\n",
        "library(\"methods\")\r\n",
        "library(\"rio\")\r\n",
        "library(\"dplyr\")\r\n",
        "library(\"stringr\")\r\n",
        "library(\"fastmatch\")\r\n",
        "library(\"lubridate\")\r\n",
        "library(\"rlang\")\r\n",
        "library(\"purrr\")\r\n",
        "library(\"jsonlite\")\r\n",
        "library(\"sf\")\r\n",
        "library(\"raster\")\r\n",
        "library(\"terra\")\r\n",
        "library(\"units\")\r\n",
        "library(\"tidyverse\")\r\n",
        "library(\"tidyr\")\r\n",
        "library(\"lwgeom\")\r\n",
        "library(\"stars\")\r\n",
        "library(\"stringr\")\r\n",
        "library(\"future\")\r\n",
        "library(\"furrr\")\r\n",
        "library(\"foreach\")\r\n",
        "library(\"doParallel\")\r\n",
        "library(\"digest\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Source Code Functions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the new control data into the stardard legacy format. ONLY REQUIRED IF \r\n",
        "# rio::import() depreciates. \r\n",
        "import_data <- function(data, index=1){\r\n",
        "  out <- tryCatch(\r\n",
        "    {\r\n",
        "      file_name <- tools::file_path_sans_ext(basename(data))\r\n",
        "      \r\n",
        "      # Opens file and stores information into dataframe. Different file types\r\n",
        "      # require different functions to read data\r\n",
        "      file_extension <- file_ext(data)\r\n",
        "      if (file_extension == 'xlsx'){\r\n",
        "        data_tibble <- read_xlsx(data, sheet = index)\r\n",
        "        data_tibble_colnames <- colnames(data_tibble)\r\n",
        "        data_df <- data.frame(data_tibble)\r\n",
        "        colnames(data_df) <- data_tibble_colnames\r\n",
        "      } else if (file_extension == 'csv'){\r\n",
        "        data_df <- read.csv(data, header = TRUE,  encoding=\"UTF-8\", check.names=FALSE)\r\n",
        "      } else {\r\n",
        "        data_df <- read.table(file=data, header=TRUE)\r\n",
        "      }\r\n",
        "      \r\n",
        "      column_names <- colnames(data_df)\r\n",
        "      column_names <- gsub(\"\\\\.\", \" \", column_names)\r\n",
        "      column_names <- gsub(\"\\\\s+\", \" \", column_names)\r\n",
        "      colnames(data_df) <- column_names\r\n",
        "      \r\n",
        "      return(data_df)\r\n",
        "    },\r\n",
        "    \r\n",
        "    error=function(cond) {\r\n",
        "      base::message(paste(\"Cannot read from:\", data))\r\n",
        "      base::message(\"Original error message:\")\r\n",
        "      base::message(cond)\r\n",
        "    }\r\n",
        "  ) \r\n",
        "  return(out)\r\n",
        "}\r\n",
        "\r\n",
        "# get vector of datetime formats to parse\r\n",
        "get_datetime_parse_order <- function(){\r\n",
        "  order <- c('%Y/%m/%d %I:%M:%S %p','%Y/%m/%d %H:%M:%S', '%d/%m/%Y %I:%M:%S %p', '%d/%m/%Y %H:%M:%S', '%Y/%m/%d %I:%M %p','%Y/%m/%d %H:%M', '%d/%m/%Y %I:%M %p','%d/%m/%Y %H:%M', '%Y-%m-%d %H:%M:%OS%z', '%Y-%m-%d %H:%M:%OS', 'ymd', 'dmy')\r\n",
        "  return(order)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "get_vessel_short_name <- function(string) {\r\n",
        "  # Function to extract short name from a single string\r\n",
        "  extract_short_name <- function(s) {\r\n",
        "    words <- strsplit(s, \"\\\\s+\")[[1]]\r\n",
        "    first_letters <- substr(words, 1, 1)\r\n",
        "    result <- paste(first_letters, collapse = \"\")\r\n",
        "    if (length(words) == 1) {\r\n",
        "      result <- words\r\n",
        "    }\r\n",
        "    return(result)\r\n",
        "  }\r\n",
        "  \r\n",
        "  # If input is a vector, apply extract_short_name to each element\r\n",
        "  if (is.vector(string)) {\r\n",
        "    sapply(string, extract_short_name)\r\n",
        "  } else {\r\n",
        "    # If input is a single string, directly apply extract_short_name\r\n",
        "    extract_short_name(string)\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "get_file_keyword <- function(string) {\r\n",
        "  \r\n",
        "  # Convert the input string to lowercase for case-insensitive matching\r\n",
        "  string <- tolower(string)\r\n",
        "  if (grepl(\"cull|dive\", string)) {\r\n",
        "    return(\"cull\")\r\n",
        "  }\r\n",
        "  if (grepl(\"manta|surveillance\", string)) {\r\n",
        "    return(\"manta_tow\")\r\n",
        "  }\r\n",
        "  if (grepl(\"rhis|survey|health\", string)) {\r\n",
        "    return(\"rhis\")\r\n",
        "  }\r\n",
        "  return(NA)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# Function to check and append records to the Vessel table. data_df is expected \r\n",
        "# to have the same column names and order as the table that is being referenced\r\n",
        "append_to_table_unique <- function(con, table_name, data_df) {\r\n",
        "  \r\n",
        "  #check what vessels exist already\r\n",
        "  db_df <- dbReadTable(con, table_name, row.names = FALSE)\r\n",
        "  unique_df <- distinct(data_df)\r\n",
        "  data_to_apppend <- anti_join(unique_df, db_df[,-which(colnames(db_df) %in% \"id\")])\r\n",
        "  if (nrow(data_to_apppend) > 0){\r\n",
        "    append_cmd  <-  sqlAppendTable( con = con , table = table_name , values = data_to_apppend , row.names = FALSE )\r\n",
        "    dbExecute( conn = con , statement = append_cmd )\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# get the Ids of rows that meet search criteria in search column \r\n",
        "get_id_by_cell <- function(con, table_name, search_column, search_term) {\r\n",
        "  entry_id <- dbGetQuery(con, paste(\"SELECT id FROM \", table_name,  \" WHERE \", search_column, \" = '\", search_term, \"'\", sep = \"\"))$id\r\n",
        "  return(entry_id)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# retrieve IDs from a database table based on matching rows in an input \r\n",
        "# dataframe, using a left join operation \r\n",
        "get_id_by_row <- function(con, table_name, data_df) {\r\n",
        "  db_df <- dbReadTable(con, table_name, row.names = FALSE)\r\n",
        "  merged_df <- left_join(data_df, db_df %>% dplyr::select(c(colnames(data_df), id)), by = colnames(data_df))\r\n",
        "  return(merged_df$id)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# get voyage dates with regex from a vector of strings. This was initially created\r\n",
        "# to ingest dates from strings in JSON files that do not have consistent formatting.\r\n",
        "get_voyage_dates_strings <- function(strings) {\r\n",
        "  \r\n",
        "  date_df <- data.frame(start_date = character(), stop_date = character(), stringsAsFactors = FALSE)\r\n",
        "  # Extract dates from each string\r\n",
        "  for (string in strings) {\r\n",
        "    dates <- str_extract_all(string, \"\\\\b\\\\d{2}/\\\\d{2}/\\\\d{4}\\\\b\")[[1]]\r\n",
        "    if (length(dates) != 2) {\r\n",
        "      dates <- c(NA, NA)\r\n",
        "    }\r\n",
        "    date_df <- rbind(date_df, setNames(as.data.frame(t(dates)), c(\"start_date\", \"stop_date\")))\r\n",
        "  }\r\n",
        "  \r\n",
        "  date_df$start_date <- parse_date_time(date_df$start_date, orders = c('dmy', 'ymd'))\r\n",
        "  date_df$stop_date <- parse_date_time(date_df$stop_date, orders = c('dmy', 'ymd'))\r\n",
        "  \r\n",
        "  return(date_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# Extract required data from Cots Control Centre databases to use as legacy dataset in reusable workflow. \r\n",
        "get_app_data_database <- function(con, control_data_type){\r\n",
        "  if(control_data_type == \"manta_tow\"){\r\n",
        "    query <- \"\r\n",
        "      SELECT manta_tow.date, vessel.name AS vessel_name, voyage.vessel_voyage_number, reef.reef_label, reef.name AS reef_name, manta_tow.start_latitude, manta_tow.stop_latitude, manta_tow.start_longitude, manta_tow.stop_longitude, manta_tow.distance, manta_tow.average_speed, manta_tow.cots, manta_tow.scars, manta_tow.hard_coral, manta_tow.soft_coral, manta_tow.recently_dead_coral, site.name AS site_name, manta_tow.error_flag\r\n",
        "      FROM manta_tow\r\n",
        "      INNER JOIN voyage ON manta_tow.voyage_id = voyage.id\r\n",
        "      INNER JOIN vessel ON voyage.vessel_id = vessel.id\r\n",
        "      INNER JOIN site ON manta_tow.site_id = site.id\r\n",
        "      INNER JOIN reef ON site.reef_id = reef.id\r\n",
        "      \"\r\n",
        "  } else if (control_data_type == \"cull\") {\r\n",
        "    query <- \"\r\n",
        "      SELECT cull.date, vessel.name AS vessel_name, voyage.vessel_voyage_number, voyage.start_date, voyage.stop_date, reef.reef_label, reef.name AS reef_name, site.latitude, site.longitude, cull.average_depth, cull.less_than_fifteen_centimetres, cull.fifteen_to_twenty_five_centimetres, cull.twenty_five_to_forty_centimetres, cull.greater_than_forty_centimetres, site.name AS site_name, cull.error_flag, cull.bottom_time\r\n",
        "      FROM cull\r\n",
        "      INNER JOIN voyage ON cull.voyage_id = voyage.id\r\n",
        "      INNER JOIN vessel ON voyage.vessel_id = vessel.id\r\n",
        "      INNER JOIN site ON cull.site_id = site.id\r\n",
        "      INNER JOIN reef ON site.reef_id = reef.id\r\n",
        "      \"\r\n",
        "  } else {\r\n",
        "    ### WRITE QUERY FOR RHIS ###\r\n",
        "    query <- \"\"\r\n",
        "  }\r\n",
        "  \r\n",
        "  result <- dbGetQuery(con, query)\r\n",
        "  return(result)\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# Parse datetimes and then extract the time component for a vector of datetimes. \r\n",
        "separate_date_time <- function(date_time){\r\n",
        "  is_date_time_na <- is.na(date_time)\r\n",
        "  date_time <- parse_date_time(date_time[!is_date_time_na], orders = get_datetime_parse_order())\r\n",
        "  time <- format(date_time, \"%H:%M:%S\")\r\n",
        "  return(time)\r\n",
        "}\r\n",
        "\r\n",
        "# Extract GBRMPA reef IDs from a vector of strings. \r\n",
        "get_reef_label <- function(names){\r\n",
        "  reef_label_pattern <- \"\\\\b(1[0-9]|2[0-9]|10)-\\\\d{3}[a-z]?\\\\b\"\r\n",
        "  reef_labels <- sapply(str_extract(names, reef_label_pattern), toString)\r\n",
        "  return(reef_labels)\r\n",
        "}\r\n",
        "\r\n",
        "#Aggregates coordinates of ecological observations that requires several trip to\r\n",
        "#survey the desired region. This specific iteration of the function utilises the \r\n",
        "#naming convention of the research format. This should be depreciated in future\r\n",
        "#in place of a single function for research and app data.\r\n",
        "get_start_and_end_coords_research <- function(start_lat, stop_lat, start_long, stop_long){\r\n",
        "  output <- get_start_and_end_coords_base(start_lat, stop_lat, start_long, stop_long)\r\n",
        "  names(output) <- c(\"Start Lat\", \"End Lat\", \"Start Lng\", \"End Lng\")\r\n",
        "  return(output)\r\n",
        "}\r\n",
        "\r\n",
        "#Aggregates coordinates of ecological observations that requires several trip to\r\n",
        "#survey the desired region. This specific iteration of the function utilises the\r\n",
        "#naming convention of the app format. This should be depreciated in future in \r\n",
        "#place of a single function for research and app data.\r\n",
        "get_start_and_end_coords_app <- function(start_lat, stop_lat, start_long, stop_long){\r\n",
        "  output <- get_start_and_end_coords_base(start_lat, stop_lat, start_long, stop_long)\r\n",
        "  names(output) <- c(\"start_latitude\", \"stop_latitude\", \"start_longitude\", \"stop_longitude\")\r\n",
        "  return(output)\r\n",
        "}\r\n",
        "\r\n",
        "#Aggregates coordinates of ecological observations that requires several trip to \r\n",
        "#survey the desired region. This specific iteration of the function is the base for both data formats.\r\n",
        "get_start_and_end_coords_base <- function(start_lat, stop_lat, start_long, stop_long){\r\n",
        "  \r\n",
        "  start_coords <- cbind(start_long, start_lat)\r\n",
        "  start_points <- lapply(1:length(start_lat), function(i) st_point(c(start_long[i], start_lat[i])))\r\n",
        "  start_points <- st_sfc(start_points)\r\n",
        "  \r\n",
        "  stop_coords <- cbind(stop_long, stop_lat)\r\n",
        "  stop_points <- lapply(1:length(stop_lat), function(i) st_point(c(stop_long[i], stop_lat[i])))\r\n",
        "  stop_points <- st_sfc(stop_points)\r\n",
        "  \r\n",
        "  distances <- st_distance(start_points,stop_points)\r\n",
        "  if(!any(is.na(distances))){\r\n",
        "    max_index <- which(distances == max(distances), arr.ind=TRUE)[1,]\r\n",
        "    start_lat <-  st_coordinates(start_points[max_index[1]])[2]\r\n",
        "    start_long <-  st_coordinates(start_points[max_index[1]])[1]\r\n",
        "    stop_lat <-  st_coordinates(stop_points[max_index[2]])[2]\r\n",
        "    stop_long <-  st_coordinates(stop_points[max_index[2]])[1]\r\n",
        "  } else {\r\n",
        "    start_lat <-  NA\r\n",
        "    start_long <-  NA\r\n",
        "    stop_lat <-  NA\r\n",
        "    stop_long <-  NA\r\n",
        "  }\r\n",
        "  output <- list(start_lat, start_long, stop_lat, stop_long)\r\n",
        "  return(output)\r\n",
        "}\r\n",
        "\r\n",
        "# Get COTS feeding scar from string decription \r\n",
        "get_feeding_scar_from_description <- function(scar_desctiptions){\r\n",
        "  return(tolower(substr(scar_desctiptions, 1, 1)))\r\n",
        "}\r\n",
        "\r\n",
        "# Get the worst observed COTS scarring as a string from a vector observed scarring.\r\n",
        "get_worst_case_feeding_scar <- function(scars){\r\n",
        "  \r\n",
        "  pattern <- \"a|p|c\"\r\n",
        "  matches <- str_extract(scars, pattern)\r\n",
        "  ordered_scars <- c(\"a\", \"p\", \"c\")\r\n",
        "  matches <- na.omit(matches)\r\n",
        "  numerical_values <- match(matches, ordered_scars)\r\n",
        "  max_vlaue <- max(numerical_values)\r\n",
        "  return(ordered_scars[max_vlaue])\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "# Extract coral cover categories from strings with regex\r\n",
        "get_coral_cover <- function(coral){\r\n",
        "  pattern <- \"(?:[1-9]\\\\-)|0|(?:(?:[1-9]\\\\+))\"\r\n",
        "  matches <- str_extract(coral, pattern)\r\n",
        "}\r\n",
        "\r\n",
        "# get median coral cover category for aggregating observations\r\n",
        "get_median_coral_cover <- function(coral){\r\n",
        "  matches <- get_coral_cover(coral)\r\n",
        "  ordered_coral_cover <- c(\"0\", \"1-\", \"1+\", \"2-\", \"2+\", \"3-\", \"3+\", \"4-\", \"4+\", \"5-\", \"5+\")\r\n",
        "  matches <- na.omit(matches)\r\n",
        "  numerical_values <- match(matches, ordered_coral_cover)\r\n",
        "  median_value <- median(numerical_values)\r\n",
        "  return(ordered_coral_cover[median_value])\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "# Which rows of a dataframe have information missing in the columns specified.\r\n",
        "missing_reef_information <- function(data, columns, test_value = c(NA)) {\r\n",
        "  results <- vector(mode = \"logical\", length = nrow(data))\r\n",
        "  for (col_name in columns) {\r\n",
        "    col_values <- data[[col_name]]\r\n",
        "    results <- results | is.na(col_values) | is.null(col_values) | col_values %in% test_value\r\n",
        "  }  \r\n",
        "  return(results)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "assign_missing_site_and_reef <- function(transformed_data_df, serialised_spatial_path, control_data_type){\r\n",
        "  # Determines if the dataframe derived from app export is missing information \r\n",
        "  # about the site or reef. Creates a geometry collection with available coordinates\r\n",
        "  # and extracts the the missing information from the RDS file. \r\n",
        "  \r\n",
        "  crs_ <- st_crs(\"+proj=longlat +datum=WGS84 +no_defs\") \r\n",
        "  if(control_data_type == \"manta_tow\"){\r\n",
        "    data_sf <- get_centroids(transformed_data_df, crs_)\r\n",
        "    cols_to_check <- c(\"Reef ID\", \"Reef\", \"Nearest Site\")  \r\n",
        "    Reef <- \"Reef\"\r\n",
        "  } else if (control_data_type == \"cull\"){\r\n",
        "    data_sf <- st_as_sf(transformed_data_df, coords=c(\"Longitude\", \"Latitude\"), crs=crs_)\r\n",
        "    cols_to_check <- c(\"Reef ID\", \"Reef Name\", \"Site Name\") \r\n",
        "    Reef <- \"Reef Name\"\r\n",
        "  } else {\r\n",
        "    data_sf <- st_as_sf(transformed_data_df, coords=c(\"Longitude\", \"Latitude\"), crs=crs_)\r\n",
        "    cols_to_check <- c(\"Reef ID\", \"Reef Name\") \r\n",
        "    Reef <- \"Reef Name\"\r\n",
        "  }\r\n",
        "  \r\n",
        "  is_missing_reef_information <- missing_reef_information(transformed_data_df, cols_to_check, c(\"Finding nearest...\"))\r\n",
        "  if (!any(is_missing_reef_information)) {\r\n",
        "    return(transformed_data_df)\r\n",
        "  }\r\n",
        "  \r\n",
        "  site_regions <- readRDS(serialised_spatial_path)\r\n",
        "  \r\n",
        "  \r\n",
        "  for (i in 1:length(site_regions)) {\r\n",
        "    current_brick <- site_regions[i][[1]]\r\n",
        "    bounding_box <- st_as_sfc(st_bbox(extent(current_brick)))\r\n",
        "    st_crs(bounding_box) <- crs_\r\n",
        "    is_within_extent <- lengths(st_within(data_sf$geometry, bounding_box)) > 0\r\n",
        "    is_within_extent_and_missing_information <- is_within_extent & is_missing_reef_information\r\n",
        "    if (any(is_within_extent_and_missing_information)){\r\n",
        "      reef_names <- names(site_regions[i])\r\n",
        "      if(control_data_type == \"manta_tow\")\r\n",
        "        transformed_data_df$Reef[is_within_extent_and_missing_information] <- reef_names\r\n",
        "      site_numbers <- raster::extract(current_brick, data_sf)[is_within_extent_and_missing_information]\r\n",
        "      site_names <- site_numbers_to_names(site_numbers, reef_names)\r\n",
        "      transformed_data_df$`Nearest Site`[is_within_extent_and_missing_information] <- site_names\r\n",
        "    } else if (control_data_type == \"cull\"){\r\n",
        "      transformed_data_df$`Reef Name`[is_within_extent_and_missing_information] <- reef_names\r\n",
        "      site_numbers <- raster::extract(current_brick, data_sf)[is_within_extent_and_missing_information]\r\n",
        "      site_names <- site_numbers_to_names(site_numbers, reef_names)\r\n",
        "      transformed_data_df$`Site Name`[is_within_extent_and_missing_information] <- site_names\r\n",
        "    } else if (control_data_type == \"rhis\"){\r\n",
        "      transformed_data_df$`Reef Name`[is_within_extent_and_missing_information] <- reef_names\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  return(transformed_data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "site_numbers_to_names <- function(numbers, reef_names){\r\n",
        "  # convert site numbers to names\r\n",
        "  \r\n",
        "  short_hand <- substr(reef_names, start = 1, stop = 3)\r\n",
        "  labels <- get_reef_label(reef_names)\r\n",
        "  site_names <- paste(toupper(short_hand), labels, numbers, sep=\"_\")\r\n",
        "}\r\n",
        "\r\n",
        "# aggregates cull data to the site level for a vessel voyage\r\n",
        "aggregate_culls_site_resolution_research <- function(data_df) {\r\n",
        "  col_names <- colnames(data_df)\r\n",
        "  \r\n",
        "  aggregated_data <- data_df  %>%\r\n",
        "    group_by(Vessel, Voyage, `Reef ID`, `Site Name`) %>%\r\n",
        "    dplyr::summarize(\r\n",
        "      `Capture ID` = first(`Capture ID`),\r\n",
        "      `Survey Date` = min(`Survey Date`),\r\n",
        "      `Reef Name` = first(`Reef Name`), \r\n",
        "      `Vessel` = Vessel, \r\n",
        "      Voyage = Voyage,\r\n",
        "      `Site Name` = `Site Name`,\r\n",
        "      `Reef ID` = `Reef ID`,\r\n",
        "      `Voyage Start` = min(`Voyage Start`),\r\n",
        "      `Voyage End` = max(`Voyage End`),\r\n",
        "      Latitude = first(Latitude),\r\n",
        "      Longitude = first(Longitude),\r\n",
        "      Bottomtime = sum(Bottomtime),\r\n",
        "      `Depth (meters)` = mean(`Depth (meters)`),\r\n",
        "      `Cohort 1 (<15cm)` = sum(`Cohort 1 (<15cm)`),\r\n",
        "      `Cohort2 (15-25cm)` = sum(`Cohort2 (15-25cm)`),\r\n",
        "      `Cohort3 (25-40cm)` = sum(`Cohort3 (25-40cm)`),\r\n",
        "      `Cohort4 (>40cm)` = sum(`Cohort4 (>40cm)`),\r\n",
        "      `Nearest Site` = `Nearest Site`,\r\n",
        "      error_flag = as.numeric(any(as.logical(error_flag)))\r\n",
        "    ) %>%\r\n",
        "    dplyr::select(all_of(col_names)) %>%\r\n",
        "    dplyr::distinct()\r\n",
        "  \r\n",
        "  \r\n",
        "  return(aggregated_data)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "aggregate_culls_site_resolution_app <- function(data_df) {\r\n",
        "  col_names <- colnames(data_df)\r\n",
        "  \r\n",
        "  aggregated_data <- data_df  %>%\r\n",
        "    group_by(vessel_name, vessel_voyage_number, reef_label, site_name) %>%\r\n",
        "    dplyr::summarize(\r\n",
        "      date = min(date),\r\n",
        "      reef_name = first(reef_name), \r\n",
        "      vessel_name = vessel_name, \r\n",
        "      vessel_voyage_number = vessel_voyage_number,\r\n",
        "      reef_label = reef_label,\r\n",
        "      start_date = min(start_date),\r\n",
        "      stop_date = max(stop_date),\r\n",
        "      latitude = first(latitude),\r\n",
        "      longitude = first(longitude),\r\n",
        "      bottom_time = sum(bottom_time),\r\n",
        "      average_depth = mean(average_depth),\r\n",
        "      less_than_fifteen_centimeters = sum(less_than_fifteen_centimeters),\r\n",
        "      fifteen_to_twenty_five_centimeters = sum(fifteen_to_twenty_five_centimeters),\r\n",
        "      twenty_five_to_forty_centimeters = sum(twenty_five_to_forty_centimeters),\r\n",
        "      greater_than_forty_centimeters = sum(greater_than_forty_centimeters),\r\n",
        "      site_name = site_name,\r\n",
        "      error_flag = as.numeric(any(as.logical(error_flag)))\r\n",
        "    ) %>%\r\n",
        "    dplyr::select(all_of(col_names)) %>%\r\n",
        "    dplyr::distinct()\r\n",
        "  \r\n",
        "  \r\n",
        "  return(aggregated_data)\r\n",
        "}\r\n",
        "\r\n",
        "  \r\n",
        "aggregate_manta_tows_site_resolution_app <- function(data_df) {\r\n",
        "  col_names <- colnames(data_df)\r\n",
        "  \r\n",
        "  aggregated_data <- data_df  %>%\r\n",
        "    group_by(vessel_name, vessel_voyage_number, reef_label, site_name) %>%\r\n",
        "    dplyr::summarize(\r\n",
        "      date = min(date),\r\n",
        "      reef_name = first(reef_name), \r\n",
        "      vessel_name = vessel_name, \r\n",
        "      vessel_voyage_number = vessel_voyage_number,\r\n",
        "      reef_label = reef_label,\r\n",
        "      coords = list(get_start_and_end_coords_app(start_latitude, start_longitude, stop_latitude, stop_latitude)),\r\n",
        "      distance = sum(distance),\r\n",
        "      average_speed = mean(average_speed),\r\n",
        "      cots = sum(cots),\r\n",
        "      scars =  get_worst_case_feeding_scar(scars),\r\n",
        "      hard_coral = get_median_coral_cover(hard_coral),\r\n",
        "      soft_coral = get_median_coral_cover(soft_coral),\r\n",
        "      recently_dead_coral = get_median_coral_cover(recently_dead_coral),\r\n",
        "      site_name = site_name,\r\n",
        "      error_flag = as.numeric(any(as.logical(error_flag))),\r\n",
        "      start_date = min(start_date),\r\n",
        "      stop_date = min(stop_date)\r\n",
        "    ) %>%\r\n",
        "    unnest_wider(coords) %>%\r\n",
        "    dplyr::select(all_of(col_names)) %>%\r\n",
        "    dplyr::distinct()\r\n",
        "  \r\n",
        "  \r\n",
        "  return(aggregated_data)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# across(c(Vessel, Voyage, `Reef ID`, `Nearest Site`), first),\r\n",
        "aggregate_manta_tows_site_resolution_research <- function(data_df) {\r\n",
        "  col_names <- colnames(data_df)\r\n",
        "  \r\n",
        "  aggregated_data <- data_df %>%\r\n",
        "    group_by(Vessel, Voyage, `Reef ID`, `Nearest Site`) %>%\r\n",
        "    dplyr::summarize(\r\n",
        "      ID = min(ID),\r\n",
        "      `Tow date` = min(`Tow date`),\r\n",
        "      `Tow Time` = min(`Tow Time`),\r\n",
        "      Reef = first(Reef), \r\n",
        "      Vessel = Vessel, \r\n",
        "      Voyage = Voyage,\r\n",
        "      `Reef ID` = `Reef ID`,\r\n",
        "      coords = list(get_start_and_end_coords_research(`Start Lat`, `Start Lng`, `End Lat`, `End Lng`)),\r\n",
        "      `Distance (metres)` = sum(`Distance (metres)`),\r\n",
        "      `Average Speed (km/h)` = mean(`Average Speed (km/h)`),\r\n",
        "      `COTS Observed` = sum(`COTS Observed`),\r\n",
        "      `Feeding Scars` =  get_worst_case_feeding_scar(`Feeding Scars`),\r\n",
        "      `Hard Coral` = get_median_coral_cover(`Hard Coral`),\r\n",
        "      `Soft Coral` = get_median_coral_cover(`Soft Coral`),\r\n",
        "      `Recently Dead Coral` = get_median_coral_cover(`Recently Dead Coral`),\r\n",
        "      `Nearest Site` = `Nearest Site`,\r\n",
        "      `error_flag` = as.numeric(any(as.logical(`error_flag`)))\r\n",
        "    ) %>%\r\n",
        "    unnest_wider(coords) %>%\r\n",
        "    dplyr::select(all_of(col_names)) %>%\r\n",
        "    dplyr::distinct()\r\n",
        "\r\n",
        "  \r\n",
        "  return(aggregated_data)\r\n",
        "} \r\n",
        "\r\n",
        "\r\n",
        "contribute_to_metadata_report <- function(key, data, parent_key=NULL, report_path=NULL){\r\n",
        "  if(is.null(report_path)){\r\n",
        "    report_path <- find_recent_file(\"Output\\\\reports\", \"Report\", \"json\")\r\n",
        "  }\r\n",
        "  if (file.exists(report_path)) {\r\n",
        "    report <- fromJSON(report_path)\r\n",
        "  } else {\r\n",
        "    print(\"report does not exist\")\r\n",
        "    return(NULL)\r\n",
        "  }\r\n",
        "  if(!is.null(parent_key)){\r\n",
        "    report[[parent_key]][[key]] <- data\r\n",
        "  } else {\r\n",
        "    report[[key]] <- data\r\n",
        "  }\r\n",
        "  toJSON(report, pretty = TRUE, auto_unbox = TRUE) %>% writeLines(report_path)\r\n",
        "}\r\n",
        "\r\n",
        "separate_control_dataframe <- function(new_data_df, legacy_data_df, has_authorative_ID){\r\n",
        "  if(any(grepl(\"ID\", colnames(new_data_df)))){\r\n",
        "    ID_col <- colnames(new_data_df)[which(grepl(\"ID\", colnames(new_data_df)))[1]]\r\n",
        "  } else {\r\n",
        "    ID_col <- \"\"\r\n",
        "  } \r\n",
        "  \r\n",
        "  column_names <- colnames(legacy_data_df)\r\n",
        "  verified_data_df <- data.frame(matrix(ncol = length(column_names), nrow = 0))\r\n",
        "  perfect_duplicates <- data.frame(matrix(ncol = length(column_names), nrow = 0))\r\n",
        "  errors_df <- data.frame(matrix(ncol = length(column_names), nrow = 0))\r\n",
        "  colnames(verified_data_df) <- column_names \r\n",
        "  colnames(perfect_duplicates) <- column_names \r\n",
        "  \r\n",
        "  # If there is a unique ID then perfect duplicates can easily be removed.\r\n",
        "  if(has_authorative_ID){\r\n",
        "    \r\n",
        "    # Find close matches of rows left without an ID and no perfect duplicates. \r\n",
        "    # Rows with a single close match will be considered a discrepancy and then\r\n",
        "    # the ID checked against the all the IDs in legacy_data_df to ensure it does \r\n",
        "    # not already exist.\r\n",
        "    \r\n",
        "    # Determine additional columns required by the new data format and remove \r\n",
        "    # from comparison\r\n",
        "    required_columns <- intersect(configuration$mappings$new_fields$field, names(new_data_df))\r\n",
        "    \r\n",
        "    # save original dataframes for legacy and new data so it can be manipulated\r\n",
        "    # without loosing data. Columns will be removed that are not going to be  \r\n",
        "    # compared for similarity. A unique identifier for each row will be created \r\n",
        "    # based on data in the row and compared. \r\n",
        "    temp_legacy_df <- legacy_data_df[,-which(colnames(legacy_data_df) %in% required_columns)]\r\n",
        "    temp_new_df <- new_data_df[,-which(colnames(new_data_df) %in% required_columns)]\r\n",
        "    \r\n",
        "    # Create a unique identifier for each row in legacy_data_df and new_data_df\r\n",
        "    temp_legacy_df$Identifier <- apply(temp_legacy_df, 1, function(row) paste(row, collapse = \"_\"))\r\n",
        "    temp_new_df$Identifier <- apply(temp_new_df, 1, function(row) paste(row, collapse = \"_\"))\r\n",
        "    \r\n",
        "    #find perfect duplicates and add to verified data df\r\n",
        "    perfect_duplicates <- legacy_data_df[temp_legacy_df$Identifier %in% temp_new_df$Identifier, ]\r\n",
        "    \r\n",
        "    # remove identifier columns\r\n",
        "    temp_legacy_df$Identifier <- NULL\r\n",
        "    temp_new_df$Identifier <- NULL\r\n",
        "    \r\n",
        "    # find new entries based on whether the ID is present in both dataframes \r\n",
        "    new_entries <- anti_join(new_data_df, legacy_data_df, by=ID_col)\r\n",
        "    \r\n",
        "    # Determine discrepancies and store both versions of the entries\r\n",
        "    non_discrepancy_ids <- c(perfect_duplicates[[ID_col]], new_entries[[ID_col]])\r\n",
        "    discrepancies_new <- new_data_df[!(new_data_df[[ID_col]] %in% non_discrepancy_ids),]\r\n",
        "    discrepancies_legacy <- legacy_data_df[!(legacy_data_df[[ID_col]] %in% non_discrepancy_ids),]\r\n",
        "    discrepancies_legacy <- discrepancies_legacy[discrepancies_legacy[[ID_col]] %in% discrepancies_new[[ID_col]], ]\r\n",
        "    verified_discrepancies <- compare_discrepancies_directly(discrepancies_new, discrepancies_legacy)\r\n",
        "    \r\n",
        "  } else {\r\n",
        "    \r\n",
        "    # Determine additional columns required by the new data format and remove \r\n",
        "    # from comparison. Even when \"Nearest Site\" exists in the legacy version we \r\n",
        "    # do not want to compare it to see if it has changed. It is recalculated \r\n",
        "    # every single workflow run through based on the available KML file.\r\n",
        "    required_columns <- intersect(c(configuration$mappings$new_fields$field, ID_col), names(new_data_df))\r\n",
        "    \r\n",
        "    # find close matching rows (distance of two) based on all columns except ID. ID is not \r\n",
        "    # because it will always be null if the data is exported from powerBI. \r\n",
        "    distance <- 3\r\n",
        "    \r\n",
        "    base::message(\"Finding close matches...\")\r\n",
        "    temp_new_df <- new_data_df[ , -which(names(new_data_df) %in% required_columns)]\r\n",
        "    temp_legacy_df <- legacy_data_df[ , -which(names(legacy_data_df) %in% required_columns)]\r\n",
        "    close_match_rows <- matrix_close_matches_vectorised(temp_legacy_df, temp_new_df, distance)\r\n",
        "    \r\n",
        "    # There can be many to many perfect matches. This means that there will be \r\n",
        "    # multiple indices referring to the same row for perfect duplicates. \r\n",
        "    # unique() should be utilised when subsetting the input dataframes.\r\n",
        "    \r\n",
        "    if(nrow(close_match_rows) > 0){\r\n",
        "      base::message(\"Close matches found.\")\r\n",
        "      separated_close_matches <- vectorised_separate_close_matches(close_match_rows)\r\n",
        "      perfect_duplicates <- new_data_df[unique(separated_close_matches$perfect[,2]),]\r\n",
        "      new_entries_i <- unique(c(separated_close_matches$discrepancies[,2],separated_close_matches$perfect[,2]))\r\n",
        "        \r\n",
        "      # This will contain any new entries and any rows that could not be separated\r\n",
        "      new_entries <- new_data_df[-new_entries_i,]\r\n",
        "\r\n",
        "      if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "        # Append the warning to an existing matrix \r\n",
        "        new_info_df <- data.frame(\r\n",
        "          ID = ifelse(ID_col != \"\", new_data_df[[ID_col]][new_entries_i],NA),\r\n",
        "          index = new_entries_i,\r\n",
        "          message = \"New Entry to the dataset. Not present in previous dataset.\"\r\n",
        "        )\r\n",
        "        duplicate_info_df <- data.frame(\r\n",
        "          ID = ifelse(ID_col != \"\", new_data_df[[ID_col]][separated_close_matches$perfect[,2]],NA),\r\n",
        "          index = separated_close_matches$perfect[,2],\r\n",
        "          message = \"Perfect duplicate. This entry was present in previous dataset.\"\r\n",
        "        )\r\n",
        "        discpreancies_info_df <- data.frame(\r\n",
        "          ID = ifelse(ID_col != \"\", new_data_df[[ID_col]][separated_close_matches$discrepancies[,2]],NA),\r\n",
        "          index = separated_close_matches$discrepancies[,2],\r\n",
        "          message = \"Discrepancy. This row has changed since the last dataset\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"New Entry\", new_info_df, parent_key = \"Data\")\r\n",
        "        contribute_to_metadata_report(\"Perfect Duplicate\", duplicate_info_df, parent_key = \"Data\")\r\n",
        "        contribute_to_metadata_report(\"Discrepancies\", discpreancies_info_df, parent_key = \"Data\")\r\n",
        "      }\r\n",
        "\r\n",
        "      verified_discrepancies <- compare_discrepancies(new_data_df, legacy_data_df, separated_close_matches$discrepancies)\r\n",
        "\r\n",
        "    } else {\r\n",
        "      base::message(\"No close matches found treating entire dataset as new\")\r\n",
        "      new_entries <- new_data_df\r\n",
        "    }\r\n",
        "    \r\n",
        "  }\r\n",
        "  \r\n",
        "  # Given that it is not possible to definitively know if a change / discrepancy \r\n",
        "  # was intentional or not both new and change entries will pass through the \r\n",
        "  # same validation checks and if passed will be accepted as usable and assumed to be . If failed, \r\n",
        "  # assumed to be a QA change. If failed,  the data will be flagged. Failed \r\n",
        "  # discrepancies will check the original legacy entry, which if failed will \r\n",
        "  # be left as is. \r\n",
        "  \r\n",
        "  verified_data_df <- rbind(verified_data_df, perfect_duplicates)\r\n",
        "  verified_data_df <- rbind(verified_data_df, verified_discrepancies)\r\n",
        "  verified_data_df <- rbind(verified_data_df, new_entries)\r\n",
        "  \r\n",
        "  # merge the verified dataset\r\n",
        "  return(verified_data_df)\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "separate_new_control_app_data <- function(new_data_df, legacy_data_df){\r\n",
        "  new_data_df <- data.frame(new_data_df, check.names = FALSE)\r\n",
        "  legacy_data_df <- data.frame(legacy_data_df, check.names = FALSE)\r\n",
        "  \r\n",
        "  column_names <- colnames(legacy_data_df)\r\n",
        "  verified_data_df <- data.frame(matrix(ncol = length(column_names), nrow = 0))\r\n",
        "  colnames(verified_data_df) <- column_names \r\n",
        "  \r\n",
        "  if(any(grepl(\"ID\", colnames(new_data_df)))){\r\n",
        "    temp_new_df <- new_data_df[ , -which(grepl(\"ID\", colnames(new_data_df)))]\r\n",
        "    temp_legacy_df <- legacy_data_df[ , -which(grepl(\"ID\", colnames(legacy_data_df)))]\r\n",
        "  } else {\r\n",
        "    temp_new_df <- new_data_df\r\n",
        "    temp_legacy_df <- legacy_data_df\r\n",
        "  } \r\n",
        "  \r\n",
        "  # find close matching rows (distance of three) based on all columns except ID. ID is not \r\n",
        "  # because it will always be null if the data is exported from powerBI. \r\n",
        "  distance <- 3\r\n",
        " \r\n",
        "  close_match_rows <- matrix_close_matches_vectorised(temp_legacy_df, temp_new_df, distance)\r\n",
        "  \r\n",
        "  # There can be many to many perfect matches. This means that there will be \r\n",
        "  # multiple indices referring to the same row for perfect duplicates. \r\n",
        "  # unique() should be utilised when subsetting the input dataframes.\r\n",
        "  if(nrow(close_match_rows) > 0){\r\n",
        "    separated_close_matches <- vectorised_separate_close_matches(close_match_rows)\r\n",
        "    print(head(separated_close_matches))\r\n",
        "    perfect_duplicates <- new_data_df[unique(separated_close_matches$perfect[,2]),]\r\n",
        "    new_entries_i <- unique(c(separated_close_matches$discrepancies[,2],separated_close_matches$perfect[,2]))\r\n",
        "    \r\n",
        "    # This will contain any new entries and any rows that could not be separated\r\n",
        "    new_entries <- new_data_df[-new_entries_i,]\r\n",
        "  } else {\r\n",
        "    new_entries <- new_data_df\r\n",
        "  }\r\n",
        "  # Given that it is not possible to definitively know if a change / discrepancy \r\n",
        "  # was intentional or not both new and change entries will pass through the \r\n",
        "  # same validation checks and if passed will be accepted as usable and assumed to be . If failed, \r\n",
        "  # assumed to be a QA change. If failed,  the data will be flagged. Failed \r\n",
        "  # discrepancies will check the original legacy entry, which if failed will \r\n",
        "  # be left as is. \r\n",
        "  \r\n",
        "  verified_data_df <- rbind(verified_data_df, new_entries)\r\n",
        "  return(verified_data_df)\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "flag_duplicates <- function(new_data_df){\r\n",
        "  # New entries need to be checked for duplicates. If there is more than one\r\n",
        "  # duplicate it can be assumed to be an error and the error flag set. This \r\n",
        "  # will use a similar identifier new_entries df. This will only flag the \r\n",
        "  # duplicate versions of the row as an error as it still contains new \r\n",
        "  # information there has just been multiple instances of data entry. \r\n",
        "  # Additionally no new entry should be a duplicate of any \"perfect duplicate\" \r\n",
        "  # as legitimate duplicates would come from the same source and uploaded at \r\n",
        "  # the same time.\r\n",
        "  \r\n",
        "  new_data_df$Identifier <- apply(new_data_df[,2:ncol(new_data_df)], 1, function(row) paste(row, collapse = \"_\"))\r\n",
        "  duplicates <- duplicated(new_data_df$Identifier)\r\n",
        "  counts <- ave(duplicates, new_data_df$Identifier, FUN = sum)\r\n",
        "  is_duplicate <- (counts >= 2 & duplicates)\r\n",
        "  new_data_df$error_flag <- ifelse(is_duplicate, 1, new_data_df$error_flag)\r\n",
        "  new_data_df$Identifier <- NULL\r\n",
        "  \r\n",
        "  if (any(is_duplicate)) {\r\n",
        "    grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "    parent <- as.character(match.call())[1]\r\n",
        "    warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have been flagged as duplicates\", \r\n",
        "                     toString(new_data_df[is_duplicate, 1]), \"and the following indexes\", toString((1:nrow(new_data_df))[is_duplicate]))\r\n",
        "    base::message(warning)\r\n",
        "    if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "      # Append the warning to an existing matrix \r\n",
        "      warnings <- data.frame(\r\n",
        "        ID = new_data_df[is_duplicate, 1],\r\n",
        "        index = which(is_duplicate),\r\n",
        "        message = \"flagged as duplicates\"\r\n",
        "      )\r\n",
        "      contribute_to_metadata_report(\"Duplicates\", warnings, parent_key = \"Warning\")\r\n",
        "    }\r\n",
        "  }\r\n",
        "  return(new_data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "compare_discrepancies <- function(new_data_df, legacy_data_df, discrepancies){\r\n",
        "  # compare the rows identified as discrepancies from the new and legacy \r\n",
        "  # dataframes. Most changes should be QA and either still meet the requirements \r\n",
        "  # or now meet the requirements and hence will not be flagged as an error. \r\n",
        "  # However a minor number of cases a row is mistakingly changed to no longer be\r\n",
        "  # useable, when this occurs the original row will be used implace of the new \r\n",
        "  # one. \r\n",
        "  \r\n",
        "  legacy_error_flag <- legacy_data_df[discrepancies[,1], \"error_flag\"]\r\n",
        "  new_error_flag <- new_data_df[discrepancies[,2], \"error_flag\"]\r\n",
        "  \r\n",
        "  is_legacy_rows <- ifelse((new_error_flag == 1) & (legacy_error_flag == 0), 1, 0)\r\n",
        "  is_legacy_rows <- as.logical(is_legacy_rows)\r\n",
        "  output <- rbind(new_data_df[discrepancies[!is_legacy_rows,2],], legacy_data_df[discrepancies[is_legacy_rows,1],])\r\n",
        "  return(output)\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "compare_discrepancies_directly <- function(new_data_df, legacy_data_df){\r\n",
        "  # compare the rows identified as discrepancies from the new and legacy \r\n",
        "  # dataframes. Most changes should be QA and either still meet the requirements \r\n",
        "  # or now meet the requirements and hence will not be flagged as an error. \r\n",
        "  # However a minor number of cases a row is mistakingly changed to no longer be\r\n",
        "  # useable, when this occurs the original row will be used implace of the new \r\n",
        "  # one. \r\n",
        "  \r\n",
        "  legacy_error_flag <- legacy_data_df[, \"error_flag\"]\r\n",
        "  new_error_flag <- new_data_df[, \"error_flag\"]\r\n",
        "  \r\n",
        "  is_legacy_rows <- ifelse((new_error_flag == 1) & (legacy_error_flag == 0), 1, 0)\r\n",
        "  is_legacy_rows <- as.logical(is_legacy_rows)\r\n",
        "  output <- rbind(new_data_df[!is_legacy_rows,], legacy_data_df[is_legacy_rows,])\r\n",
        "  return(output)\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "vectorised_separate_close_matches <- function(close_match_rows){\r\n",
        "  # Separate the close matching rows with a vectorized process. Duplicates in \r\n",
        "  # columns of `close_match_rows` indicates that there are multiple close \r\n",
        "  # matches between a row(s) in dataframe and and row(s) in y. This process \r\n",
        "  # utilised logical checks on vectors or matrices so Boolean operations can be \r\n",
        "  # utilsied to separate the rows. Although this does reduce readability, the \r\n",
        "  # reduced computational time of two or three orders of magnitude was deemed a \r\n",
        "  # worthy trade off. \r\n",
        "  \r\n",
        "  # Order of matches handled\r\n",
        "  # 1) Handle rows with one-one close matches \r\n",
        "  # 2) Handle rows with many-many perfect matches \r\n",
        "  # 3) Handle rows with one-many perfect matches\r\n",
        "  # 4) Handle rows with one-many nonperfect matches\r\n",
        "  # 5) Handle rows with many-many nonperfect matches\r\n",
        "  # 6) Handle rows with one-one and one-many nonperfect matches\r\n",
        "  \r\n",
        "  # This order MATTERS. This is NOT a definitive process and therefore the order\r\n",
        "  # needs to maximize the probability that a row from the new data set is \r\n",
        "  # matched with one from the previous data set. One to one matches and\r\n",
        "  # many-many perfect matches are the most likely to be correct and therefore \r\n",
        "  # are removed first. It is important that the next matches handled are\r\n",
        "  # one-many. This is to ensure a match is found for the \"one\", as its most \r\n",
        "  # likely match is one of the many with the smallest distance. Any rows with \r\n",
        "  # those indices can then be removed to prevent double handling. Once many-many \r\n",
        "  # rows have been handled, one-one or one-many relationships may have been \r\n",
        "  # formed and therefore can be handled repetitively until all matches have been \r\n",
        "  # found or no more can be found\r\n",
        "  \r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #Initialise variables \r\n",
        "  \r\n",
        "  x_df_col <- 1\r\n",
        "  y_df_col <- 2\r\n",
        "  distance <- max(close_match_rows[,3])\r\n",
        "  \r\n",
        "  # initialise variables to store indices of rows after seperation\r\n",
        "  discrepancies_indices <<- matrix(,ncol=3, nrow =0)\r\n",
        "  perfect_duplicate_indices <<- matrix(,ncol=3, nrow =0)\r\n",
        "  error_indices <<- matrix(,ncol=3, nrow =0)\r\n",
        "  mistake_duplicates <- matrix(,ncol=3, nrow =0)\r\n",
        "  one_to_many_e <- matrix(,ncol=3, nrow =0)\r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #one-one close matches\r\n",
        "  \r\n",
        "  close_match_rows_updated <- find_one_to_one_matches(close_match_rows)\r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #one-many perfect matches\r\n",
        "  \r\n",
        "  x_updated_dup_indices <- (duplicated(close_match_rows_updated[,x_df_col])|duplicated(close_match_rows_updated[,x_df_col], fromLast=TRUE))\r\n",
        "  y_updated_dup_indices <- (duplicated(close_match_rows_updated[,y_df_col])|duplicated(close_match_rows_updated[,y_df_col], fromLast=TRUE))\r\n",
        "  \r\n",
        "  # condition finds rows in `close_match_rows_updated` where only one column is a duplicate\r\n",
        "  perfect_one_to_many <- !(y_updated_dup_indices & x_updated_dup_indices) & (y_updated_dup_indices | x_updated_dup_indices) & (close_match_rows_updated[,3] == 0)\r\n",
        "  \r\n",
        "  # Only select one match for a row\r\n",
        "  perfect_one_to_many_matches <- close_match_rows_updated[perfect_one_to_many,, drop = FALSE]\r\n",
        "  x_indices <- perfect_one_to_many_matches[,x_df_col]\r\n",
        "  y_indices <- perfect_one_to_many_matches[,y_df_col]\r\n",
        "  x_dup_indices <- (duplicated(x_indices)|duplicated(x_indices, fromLast=TRUE))\r\n",
        "  y_dup_indices <- (duplicated(y_indices)|duplicated(y_indices, fromLast=TRUE))\r\n",
        "  dup_indices <- (y_dup_indices | x_dup_indices)\r\n",
        "  non_dup_indices <- !dup_indices\r\n",
        "  \r\n",
        "  perfect_duplicate_indices <- rbind(perfect_duplicate_indices, perfect_one_to_many_matches[non_dup_indices,])\r\n",
        "  error_indices <- rbind(error_indices, perfect_one_to_many_matches[dup_indices,])\r\n",
        "  \r\n",
        "  # remove checked rows\r\n",
        "  close_match_rows_updated <- close_match_rows_updated[-unique(c(which(close_match_rows_updated[,x_df_col] %fin% x_indices), which(close_match_rows_updated[,y_df_col] %fin% y_indices))),]\r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #Re-check one-one close matches\r\n",
        "  close_match_rows_updated <- find_one_to_one_matches(close_match_rows_updated)\r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #many-many perfect matches\r\n",
        "  \r\n",
        "  # initialize variables for many to many separation\r\n",
        "  one_to_one_indices <- c()\r\n",
        "  one_to_many_indices <- c()\r\n",
        "  many_to_many_indices<- c()\r\n",
        "  mistake_duplicates_indices <- c()\r\n",
        "  \r\n",
        "  # Boolean logic to subset many-many perfect matches from the `close_match_rows`\r\n",
        "  # matrix\r\n",
        "  is_perfect_match <- close_match_rows_updated[,3] == 0\r\n",
        "  x_updated_dup_indices <- (duplicated(close_match_rows_updated[,x_df_col])|duplicated(close_match_rows_updated[,x_df_col], fromLast=TRUE))\r\n",
        "  y_updated_dup_indices <- (duplicated(close_match_rows_updated[,y_df_col])|duplicated(close_match_rows_updated[,y_df_col], fromLast=TRUE))\r\n",
        "  many_to_many <- (y_updated_dup_indices & x_updated_dup_indices) \r\n",
        "  many_to_many_with_perfect_match <- many_to_many & is_perfect_match\r\n",
        "  \r\n",
        "  # many-many with perfect matches have three possible scenarios. \r\n",
        "  # x_df_col) A single perfect matching row with multiple close matches\r\n",
        "  # 2) Two perfect matching rows (This is considered coincidental and both are\r\n",
        "  #     treated as unique)\r\n",
        "  # 3) more than two perfect matching rows (This is considered a mistake and \r\n",
        "  #     flagged)\r\n",
        "  # All scenarios need to be checked against.\r\n",
        "  \r\n",
        "  # update the relevant matrices if matches are found. The indices with \r\n",
        "  # multiple perfect matches may also have other matches that are nonperfect \r\n",
        "  # and therefore need to be filtered again to ensure only perfect is \r\n",
        "  # considered\r\n",
        "  \r\n",
        "  if(any(many_to_many_with_perfect_match)){\r\n",
        "    \r\n",
        "    many_to_many_with_perfect_match_entries <- close_match_rows_updated[many_to_many_with_perfect_match,, drop = FALSE]\r\n",
        "    \r\n",
        "    x_dup_indices <- (duplicated(many_to_many_with_perfect_match_entries[,x_df_col])|duplicated(many_to_many_with_perfect_match_entries[,x_df_col], fromLast=TRUE))\r\n",
        "    y_dup_indices <- (duplicated(many_to_many_with_perfect_match_entries[,y_df_col])|duplicated(many_to_many_with_perfect_match_entries[,y_df_col], fromLast=TRUE))\r\n",
        "    \r\n",
        "    \r\n",
        "    many_to_many_i <- x_dup_indices & y_dup_indices\r\n",
        "    one_to_many_i <- !(x_dup_indices & y_dup_indices) & (y_dup_indices | x_dup_indices)\r\n",
        "    one_to_one_i <- !(x_dup_indices | y_dup_indices)\r\n",
        "    \r\n",
        "    # argument , drop = FALSE keeps as a matrix even if it is only a single row. \r\n",
        "    many_to_many_e <- many_to_many_with_perfect_match_entries[many_to_many_i,, drop = FALSE]\r\n",
        "    one_to_many_e <- many_to_many_with_perfect_match_entries[one_to_many_i,, drop = FALSE]\r\n",
        "    one_to_one_e <- many_to_many_with_perfect_match_entries[one_to_one_i,, drop = FALSE]\r\n",
        "    \r\n",
        "    # separate groups of many to many matches. There should not be one to many \r\n",
        "    # relationship of perfect matches, if there is there are a combination of \r\n",
        "    # discrepancies which will not be able to be definitively determined and \r\n",
        "    # therefore will be set as new entries. \r\n",
        "    \r\n",
        "    if(nrow(many_to_many_e) > 0){\r\n",
        "      \r\n",
        "      my_df_colm_split <- lapply(split(many_to_many_e[,x_df_col:y_df_col], many_to_many_e[,y_df_col]), matrix, ncol=y_df_col)\r\n",
        "      \r\n",
        "      groups <- replicate(length(my_df_colm_split), vector(), simplify = FALSE)\r\n",
        "      group <- 0\r\n",
        "      output_rec_group <- rec_group(my_df_colm_split, groups, group)\r\n",
        "      \r\n",
        "      # Now that the many to many perfect matches are grouped, if they are\r\n",
        "      # genuine perfect matches with no mistakes there will be as many duplicate\r\n",
        "      # vectors as their are matching rows. Any with more than 2 matching rows \r\n",
        "      # as previously discussed will be removed and considered mistake \r\n",
        "      # duplicates. A pair of perfect duplicates will be considered a \r\n",
        "      # coincidence and kept \r\n",
        "      \r\n",
        "      perfect_matching_many_to_many_rows <- (duplicated(output_rec_group)|duplicated(output_rec_group, fromLast=TRUE))\r\n",
        "      too_many_matches <- unlist(lapply(output_rec_group, function(x) length(x) > 2))\r\n",
        "      too_many_matches_indices <- unique(unlist(output_rec_group[too_many_matches]))\r\n",
        "      \r\n",
        "      too_few_matches <- unlist(lapply(output_rec_group, function(x) length(x) < 2))\r\n",
        "      too_few_matches_indices <- unique(unlist(output_rec_group[too_few_matches]))\r\n",
        "      \r\n",
        "      new_many_to_many <- unique(many_to_many_e[,y_df_col])\r\n",
        "      incorrect_many_to_many <- new_many_to_many[new_many_to_many %fin% too_few_matches_indices | new_many_to_many %fin% too_many_matches_indices]\r\n",
        "      correct_many_to_many <- new_many_to_many[!(new_many_to_many %fin% too_few_matches_indices | new_many_to_many %fin% too_many_matches_indices)]\r\n",
        "      \r\n",
        "      \r\n",
        "      mistake_duplicate_manye_indices <- which(many_to_many_e[,y_df_col] %fin% incorrect_many_to_many)\r\n",
        "      mistake_duplicates <- many_to_many_e[mistake_duplicate_manye_indices,]\r\n",
        "      mistake_duplicates_indices <- unique(c(which(close_match_rows_updated[,y_df_col] %fin% mistake_duplicates[,y_df_col]), which(close_match_rows_updated[,x_df_col] %fin% mistake_duplicates[,x_df_col])))\r\n",
        "      many_to_many_e <- many_to_many_e[-mistake_duplicate_manye_indices,]\r\n",
        "      many_to_many_indices <- unique(c(which(close_match_rows_updated[,y_df_col] %fin% many_to_many_e[,y_df_col]), which(close_match_rows_updated[,x_df_col] %fin% many_to_many_e[,x_df_col])))\r\n",
        "      \r\n",
        "    }\r\n",
        "    \r\n",
        "    if(nrow(one_to_many_e) > 0){ \r\n",
        "      one_to_many_indices <- unique(c(which(close_match_rows_updated[,y_df_col] %fin% one_to_many_e[,y_df_col]), which(close_match_rows_updated[,x_df_col] %fin% one_to_many_e[,x_df_col])))\r\n",
        "    \r\n",
        "      }\r\n",
        "        \r\n",
        "    one_to_one_indices <- unique(c(which(close_match_rows_updated[,y_df_col] %fin% one_to_one_e[,y_df_col]), which(close_match_rows_updated[,x_df_col] %fin% one_to_one_e[,x_df_col])))\r\n",
        "    \r\n",
        "    perfect_duplicate_indices <- rbind(perfect_duplicate_indices, one_to_one_e, many_to_many_e)\r\n",
        "    error_indices <- rbind(error_indices, mistake_duplicates, one_to_many_e)\r\n",
        "    \r\n",
        "    # remove rows that have already been handled to prevent double handling.\r\n",
        "    close_match_rows_updated <- close_match_rows_updated[-unique(c(one_to_one_indices, one_to_many_indices, many_to_many_indices, mistake_duplicates_indices)),]\r\n",
        "    \r\n",
        "  }\r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #Re-check one-one close matches\r\n",
        "  close_match_rows_updated <- find_one_to_one_matches(close_match_rows_updated)\r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #one-many nonperfect matches\r\n",
        "  \r\n",
        "  # Iterates starting with matches of the closest distance to ensure they are \r\n",
        "  # given priority. \r\n",
        "  for(i in 1:distance){\r\n",
        "    x_dup_indices <- (duplicated(close_match_rows_updated[,x_df_col])|duplicated(close_match_rows_updated[,x_df_col], fromLast=TRUE))\r\n",
        "    y_dup_indices <- (duplicated(close_match_rows_updated[,y_df_col])|duplicated(close_match_rows_updated[,y_df_col], fromLast=TRUE))\r\n",
        "    \r\n",
        "    # condition finds rows in `close_match_rows_updated` where only one column is a duplicate\r\n",
        "    one_to_many <- !(y_dup_indices & x_dup_indices) & (y_dup_indices | x_dup_indices) & close_match_rows_updated[,3] == i\r\n",
        "    \r\n",
        "    # Check that the same row is not being matched to multiple. If they are \r\n",
        "    # them and they will be handled at the end by being treated as new rows\r\n",
        "    matches <- close_match_rows_updated[one_to_many,]\r\n",
        "    if(!is.null(nrow(matches))){\r\n",
        "      match_x_dup_indices <- (duplicated(matches[,x_df_col])|duplicated(matches[,x_df_col], fromLast=TRUE))\r\n",
        "      match_y_dup_indices <- (duplicated(matches[,y_df_col])|duplicated(matches[,y_df_col], fromLast=TRUE))\r\n",
        "      discrepancies_indices <- rbind(discrepancies_indices, matches[!(match_y_dup_indices | match_x_dup_indices),])\r\n",
        "      error_indices <- rbind(error_indices, matches[(match_y_dup_indices | match_x_dup_indices),])\r\n",
        "      \r\n",
        "      # remove checked rows and any that have already been matched. \r\n",
        "      close_match_rows_updated <- close_match_rows_updated[-unique(c(which(close_match_rows_updated[,x_df_col] %fin% matches[,x_df_col]), which(close_match_rows_updated[,y_df_col] %fin% matches[,y_df_col]))),]\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #Re-check one-one close matches\r\n",
        "  close_match_rows_updated <- find_one_to_one_matches(close_match_rows_updated)\r\n",
        "  \r\n",
        "  ### ---------- \r\n",
        "  #many-many nonperfect matches\r\n",
        "  \r\n",
        "  x_dup_indices <- (duplicated(close_match_rows_updated[,x_df_col])|duplicated(close_match_rows_updated[,x_df_col], fromLast=TRUE))\r\n",
        "  y_dup_indices <- (duplicated(close_match_rows_updated[,y_df_col])|duplicated(close_match_rows_updated[,y_df_col], fromLast=TRUE))\r\n",
        "  \r\n",
        "  \r\n",
        "  # Only many-to-many non perfect matches are left and need to be handled. \r\n",
        "  for(i in 1:distance){\r\n",
        "    if(nrow(close_match_rows_updated) > 0){\r\n",
        "      many_to_man_with_nonperfect_match <- y_dup_indices & x_dup_indices & (close_match_rows_updated[,3] == i)\r\n",
        "      many_to_many_with_nonperfect_match_entries <- close_match_rows_updated[many_to_man_with_nonperfect_match,]\r\n",
        "      \r\n",
        "      x_dup_indices <- (duplicated(many_to_many_with_nonperfect_match_entries[,x_df_col])|duplicated(many_to_many_with_nonperfect_match_entries[,x_df_col], fromLast=TRUE))\r\n",
        "      y_dup_indices <- (duplicated(many_to_many_with_nonperfect_match_entries[,y_df_col])|duplicated(many_to_many_with_nonperfect_match_entries[,y_df_col], fromLast=TRUE))\r\n",
        "      \r\n",
        "      \r\n",
        "      many_to_many_i <- x_dup_indices & y_dup_indices\r\n",
        "      one_to_many_i <- !(y_updated_dup_indices & x_updated_dup_indices) & (y_updated_dup_indices | x_updated_dup_indices)\r\n",
        "      one_to_one_i <- !(y_updated_dup_indices | x_updated_dup_indices)\r\n",
        "      \r\n",
        "      \r\n",
        "      many_to_many_e <- many_to_many_with_nonperfect_match_entries[many_to_many_i,,drop = FALSE]\r\n",
        "      one_to_many_e <- many_to_many_with_nonperfect_match_entries[one_to_many_i,,drop = FALSE]\r\n",
        "      one_to_one_e <- many_to_many_with_nonperfect_match_entries[one_to_one_i,,drop = FALSE]\r\n",
        "      \r\n",
        "      if(sum(nrow(many_to_many_e), nrow(one_to_many_e), nrow(one_to_one_e)) > 0){\r\n",
        "        \r\n",
        "        \r\n",
        "        many_to_many_indices <- unique(c(which(close_match_rows_updated[,y_df_col] %fin% many_to_many_e[,y_df_col]), which(close_match_rows_updated[,x_df_col] %fin% many_to_many_e[,x_df_col])))\r\n",
        "        one_to_many_indices <- unique(c(which(close_match_rows_updated[,y_df_col] %fin% one_to_many_e[,y_df_col]), which(close_match_rows_updated[,x_df_col] %fin% one_to_many_e[,x_df_col])))\r\n",
        "        one_to_one_indices <- unique(c(which(close_match_rows_updated[,y_df_col] %fin% one_to_one_e[,y_df_col]), which(close_match_rows_updated[,x_df_col] %fin% one_to_one_e[,x_df_col])))\r\n",
        "        \r\n",
        "        perfect_duplicate_indices <- rbind(perfect_duplicate_indices, one_to_one_e)\r\n",
        "        error_indices <- rbind(error_indices, one_to_many_e, many_to_many_e)\r\n",
        "        \r\n",
        "        # remove rows that have already been handled to prevent double handling.\r\n",
        "        close_match_rows_updated <- close_match_rows_updated[-unique(c(one_to_one_indices, one_to_many_indices, many_to_many_indices)),]\r\n",
        "      }\r\n",
        "    } else{\r\n",
        "      next\r\n",
        "    }\r\n",
        "  }\r\n",
        "  ### ---------- \r\n",
        "  #one-one and one-many nonperfect matches \r\n",
        "  \r\n",
        "  # Handling one to many relationship or many to many with discrepancies may  \r\n",
        "  # have left a number of one to one or one to many rows. This will be iterative \r\n",
        "  # until none remain or no further matches can be found. Anything left will be assigned to  \r\n",
        "  # new entry to ensure that it is processed correctly.\r\n",
        "  is_unmatched <- nrow(close_match_rows_updated) > 0 \r\n",
        "  while(is_unmatched){\r\n",
        "    count <- nrow(close_match_rows_updated)\r\n",
        "    \r\n",
        "    close_match_rows_updated <- find_one_to_one_matches(close_match_rows)\r\n",
        "    \r\n",
        "    # condition finds rows in `close_match_rows_updated` where only one column is a duplicate\r\n",
        "    for(i in 1:distance){\r\n",
        "      if(nrow(close_match_rows_updated) == 0){\r\n",
        "        break\r\n",
        "      }\r\n",
        "      x_dup_indices <- (duplicated(close_match_rows_updated[,x_df_col])|duplicated(close_match_rows_updated[,x_df_col], fromLast=TRUE))\r\n",
        "      y_dup_indices <- (duplicated(close_match_rows_updated[,y_df_col])|duplicated(close_match_rows_updated[,y_df_col], fromLast=TRUE))\r\n",
        "      one_to_many <- !(y_dup_indices & x_dup_indices) & (y_dup_indices | x_dup_indices) & close_match_rows_updated[,3] == i\r\n",
        "      \r\n",
        "      # Check that the same row is not being matched to multiple. If they are \r\n",
        "      # them and they will be handled at the end by being treated as new rows\r\n",
        "      matches <- close_match_rows_updated[one_to_many,]\r\n",
        "      if(nrow(matches) > 0){\r\n",
        "        match_x_dup_indices <- (duplicated(matches[,x_df_col])|duplicated(matches[,x_df_col], fromLast=TRUE))\r\n",
        "        match_y_dup_indices <- (duplicated(matches[,y_df_col])|duplicated(matches[,y_df_col], fromLast=TRUE))\r\n",
        "        discrepancies_indices <- rbind(discrepancies_indices, matches[!(match_y_dup_indices | match_x_dup_indices),])\r\n",
        "        error_indices <- rbind(error_indices, matches[(match_y_dup_indices | match_x_dup_indices),])\r\n",
        "        \r\n",
        "        # remove checked rows and any that have already been matched. \r\n",
        "        close_match_rows_updated <- close_match_rows_updated[-unique(c(which(close_match_rows_updated[,x_df_col] %fin% matches[,x_df_col]), which(close_match_rows_updated[,y_df_col] %fin% matches[,y_df_col]))),]\r\n",
        "        \r\n",
        "      }\r\n",
        "      is_unmatched <- !((count - nrow(close_match_rows_updated) == 0) | nrow(close_match_rows_updated) == 0)\r\n",
        "      \r\n",
        "    }\r\n",
        "  }\r\n",
        "  # check for mistakes \r\n",
        "  is_mistake_present <- check_for_mistake(control_data_type)\r\n",
        "  \r\n",
        "  output <- list(discrepancies_indices, perfect_duplicate_indices, error_indices)\r\n",
        "  names(output) <- c(\"discrepancies\", \"perfect\", \"error\") \r\n",
        "  return(output)\r\n",
        "}\r\n",
        "\r\n",
        "check_for_mistake <- function(control_data_type){\r\n",
        "  x_df_col <- 1\r\n",
        "  y_df_col <- 2\r\n",
        "  \r\n",
        "  # Check that there are no duplicates between data frames\r\n",
        "  elements_count_x <- table(c(unique(discrepancies_indices[,x_df_col]),unique(perfect_duplicate_indices[,x_df_col]),unique(error_indices[,x_df_col])))\r\n",
        "  common_x_values <- names(elements_count_x[elements_count_x >= 2])\r\n",
        "  \r\n",
        "  elements_count_y <- table(c(unique(discrepancies_indices[,y_df_col]),unique(perfect_duplicate_indices[,y_df_col]),unique(error_indices[,y_df_col])))\r\n",
        "  common_y_values <- names(elements_count_y[elements_count_y >= 2])\r\n",
        "  \r\n",
        "  if (any(common_y_values == 0)| any(common_x_values == 0)) {\r\n",
        "    grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "    parent <- as.character(match.call())[1]\r\n",
        "    warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows were not correctly separated and the following row indexes were labeled as a `Perfect Duplicate`, `Discrepancy` and/or an `Error`:\", \r\n",
        "                     \"New Data Indices (Y):\", toString(common_y_values), \"Legacy Data Indices (X):\", toString(common_x_values))\r\n",
        "    base::message(warning)\r\n",
        "    \r\n",
        "    if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "      # Append the warning to an existing matrix \r\n",
        "      warnings <- data.frame(\r\n",
        "        index_new = common_y_values,\r\n",
        "        index_legacy = common_x_values,\r\n",
        "        message = \"Row with listed indices were incorrectly separated\"\r\n",
        "      )\r\n",
        "      contribute_to_metadata_report(\"Separation\", warnings, parent_key = \"Warning\")\r\n",
        "    }\r\n",
        "    \r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# Run all verification functions on data sets. All verification functions are \r\n",
        "# within try catch to ensure that a fatal error will not break the workflow. \r\n",
        "verify_entries <- function(data_df, configuration){\r\n",
        "  control_data_type <- configuration$metadata$control_data_type\r\n",
        "  base::message(\"HEREREREEEEEEEEEEEEEEEEEE\")\r\n",
        "  data_df <- verify_integers_positive(data_df)\r\n",
        "  base::message(\"HEREREREEEEEEEEEEEEEEEEEE\")\r\n",
        "  data_df <- verify_reef(data_df)\r\n",
        "  base::message(\"HEREREREEEEEEEEEEEEEEEEEE\")\r\n",
        "  data_df <- verify_percentages(data_df)\r\n",
        "  base::message(\"HEREREREEEEEEEEEEEEEEEEEE\")\r\n",
        "  #verify long and lat separately\r\n",
        "  data_df <- verify_lat_lng(data_df, max_val=160, min_val=138, columns=c(\"Longitude\", \"Start Lng\", \"End Lng\"))\r\n",
        "  data_df <- verify_lat_lng(data_df, max_val=-5, min_val=-32, columns=c(\"Latitude\", \"Start Lat\", \"End Lat\"))\r\n",
        "  base::message(\"HEREREREEEEEEEEEEEEEEEEEE\")\r\n",
        "  if (control_data_type == \"manta_tow\") {\r\n",
        "    data_df <- verify_tow_date(data_df)\r\n",
        "    data_df <- verify_coral_cover(data_df)\r\n",
        "    data_df <- verify_scar(data_df)\r\n",
        "  } else if (control_data_type == \"cull\") {\r\n",
        "    data_df <- verify_voyage_dates(data_df)\r\n",
        "  } else if (control_data_type == \"RHISS\") {\r\n",
        "    data_df <- verify_RHISS(data_df)\r\n",
        "  } \r\n",
        "  data_df <- verify_na_null(data_df, configuration)\r\n",
        "  data_df <- verify_available_columns(data_df, configuration)\r\n",
        "  data_df$error_flag <- as.integer(data_df$error_flag)\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "verify_available_columns <- function(data_df, configuration){\r\n",
        "  \r\n",
        "  tryCatch({\r\n",
        "    transformations <- configuration$mappings$transformations\r\n",
        "    nonexempt_cols <- transformations[transformations$verify_na_exempt == FALSE, \"target_field\"]\r\n",
        "    is_nonexempt_cols_available <- !all(nonexempt_cols %in% colnames(data_df))\r\n",
        "    if (is_nonexempt_cols_available){\r\n",
        "      data_df[[\"error_flag\"]] <- as.integer(is_nonexempt_cols_available)\r\n",
        "    }\r\n",
        "    \r\n",
        "    if (is_nonexempt_cols_available) {\r\n",
        "      errors <- data.frame(\r\n",
        "        verification_function = \"is_nonexempt_cols_available\",\r\n",
        "        message = as.character(e)\r\n",
        "      )\r\n",
        "      contribute_to_metadata_report(\"is_nonexempt_cols_available\", errors, parent_key = \"Columns missing\")\r\n",
        "    }\r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_available_columns\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_available_columns\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "verify_lat_lng <- function(data_df, max_val, min_val, columns){\r\n",
        "  tryCatch({\r\n",
        "    for (col in columns) {\r\n",
        "      if (col %in% colnames(data_df)) {\r\n",
        "        values <- data_df[[col]]\r\n",
        "        out_of_range <- values < min_val | values > max_val\r\n",
        "        \r\n",
        "        # Set any NA values to TRUE as the check was unable to be completed \r\n",
        "        # correctly\r\n",
        "        out_of_range <- ifelse(is.na(out_of_range), FALSE, out_of_range)\r\n",
        "        data_df[[\"error_flag\"]] <- as.integer(data_df[[\"error_flag\"]] | out_of_range)\r\n",
        "        \r\n",
        "        if (any(out_of_range)) {\r\n",
        "          grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "          parent <- as.character(match.call())[1]\r\n",
        "          warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have inappropriate LatLong coordinates:\", \r\n",
        "                           toString(data_df[out_of_range, 1]), \"and the following indexes\", toString((1:nrow(data_df))[out_of_range]))\r\n",
        "          base::message(warning)\r\n",
        "          \r\n",
        "          if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "            # Append the warning to an existing matrix \r\n",
        "            warnings <- data.frame(\r\n",
        "              ID = data_df[out_of_range, 1],\r\n",
        "              index = which(out_of_range),\r\n",
        "              message = \"Inappropriate Lat Long values\"\r\n",
        "            )\r\n",
        "            contribute_to_metadata_report(\"Coordinates\", warnings, parent_key = \"Warning\")\r\n",
        "          }\r\n",
        "        }\r\n",
        "      }\r\n",
        "    }\r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_lat_lng\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_lat_lng\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "verify_scar <- function(data_df) {\r\n",
        "  tryCatch({\r\n",
        "    # check that columns in RHISS data contain expected values according to metadata\r\n",
        "    valid_scar <- c(\"a\", \"p\", \"c\")\r\n",
        "    \r\n",
        "    col_names <- colnames(data_df)\r\n",
        "    col_names <- tolower(col_names)\r\n",
        "    search_word <- \"scar\"\r\n",
        "    matching_columns <- col_names[grepl(search_word, col_names)]\r\n",
        "    \r\n",
        "    for (col in matching_columns) {\r\n",
        "      check_valid_scar <- data_df[[col]] %in% valid_scar\r\n",
        "      is_not_valid <- ifelse(is.na(check_valid_scar), TRUE, check_valid_scar)\r\n",
        "      is_not_valid <- !is_not_valid\r\n",
        "      data_df[[\"error_flag\"]] <- as.integer(data_df[[\"error_flag\"]] | is_not_valid)\r\n",
        "      \r\n",
        "      if (any(is_not_valid )) {\r\n",
        "        grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "        parent <- as.character(match.call())[1]\r\n",
        "        warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have invalid COTS scar:\",\r\n",
        "                         toString(data_df[is_not_valid , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[is_not_valid]))\r\n",
        "        base::message(warning)\r\n",
        "        \r\n",
        "        \r\n",
        "        if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "          # Append the warning to an existing matrix \r\n",
        "          warnings <- data.frame(\r\n",
        "            ID = data_df[is_not_valid, 1],\r\n",
        "            index = which(is_not_valid),\r\n",
        "            message = \"Invalid COT Scar\"\r\n",
        "          )\r\n",
        "          contribute_to_metadata_report(col, warnings, parent_key = \"Warning\")\r\n",
        "        }\r\n",
        "        \r\n",
        "      }\r\n",
        "    }\r\n",
        "    \r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_scar\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_scar\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "verify_tow_date <- function(data_df){\r\n",
        "  tryCatch({\r\n",
        "    # Approximate a tow date based on vessel and voyage if it does not exist. \r\n",
        "    \r\n",
        "    tow_date <- data_df[[\"Tow date\"]]\r\n",
        "    incomplete_dates <- unique(which(is.na(tow_date)))\r\n",
        "    if (length(incomplete_dates) > 0){\r\n",
        "      incomplete_date_rows <- data_df[incomplete_dates,]\r\n",
        "      vessel_voyage <- unique(incomplete_date_rows[,which(names(incomplete_date_rows) %in% c(\"Vessel\", \"Voyage\"))])\r\n",
        "      for(i in 1:nrow(vessel_voyage)){\r\n",
        "        data_df_filtered <- data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]),]\r\n",
        "        is_any_voyage_date_correct <- any(!is.na(data_df_filtered$`Tow date`))\r\n",
        "        if(is_any_voyage_date_correct){\r\n",
        "          correct_date <- data_df_filtered[!(is.na(data_df_filtered$`Tow date`) | is.null(data_df_filtered$`Tow date`)),][1,\"Tow date\"]\r\n",
        "          data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]) & (is.na(data_df$`Tow date`)), \"Tow date\"] <- correct_date\r\n",
        "        } else if(!is_any_voyage_date_correct & !is_any_survey_date_correct) {\r\n",
        "          data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]) & (is.na(data_df$`Tow date`)), c(\"error_flag\")] <- 1\r\n",
        "          \r\n",
        "        }\r\n",
        "      }\r\n",
        "      \r\n",
        "    }\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    post_estimation_tow_dates <- data_df[[\"Tow date\"]]\r\n",
        "    na_present <- (is.na(post_estimation_tow_dates) | is.null(post_estimation_tow_dates)) & (is.na(tow_date) | is.null(tow_date))\r\n",
        "    dated_estimated <- !(is.na(post_estimation_tow_dates) | is.null(post_estimation_tow_dates)) & (is.na(tow_date) | is.null(tow_date))\r\n",
        "    \r\n",
        "    if (any(dated_estimated | na_present)) {\r\n",
        "      grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "      parent <- as.character(match.call())[1]\r\n",
        "      warning <- c()\r\n",
        "      if (any(dated_estimated)) {\r\n",
        "        warning1 <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have their tow date estimated from their vessel\",\r\n",
        "                          toString(data_df[dated_estimated , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[dated_estimated]))\r\n",
        "        base::message(warning1)\r\n",
        "      }\r\n",
        "      if (any(na_present)) {\r\n",
        "        warning2 <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have no tow date\",\r\n",
        "                          toString(data_df[na_present , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[na_present]))\r\n",
        "        base::message(warning2)\r\n",
        "      }  \r\n",
        "      \r\n",
        "      \r\n",
        "      if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "        # Append the warning to an existing matrix \r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[dated_estimated, 1],\r\n",
        "          index = which(dated_estimated),\r\n",
        "          message = \"Invalid Tow Date. Date was successfully estimated.\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Estimated Tow Date\", warnings, parent_key = \"Warning\")\r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[na_present, 1],\r\n",
        "          index = which(na_present),\r\n",
        "          message = \"Invalid Tow Date. \"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Invalid Tow Date\", warnings, parent_key = \"Warning\")\r\n",
        "      }\r\n",
        "      \r\n",
        "    }\r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_tow_date\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_tow_date\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "verify_RHISS <- function(data_df) {\r\n",
        "  tryCatch({\r\n",
        "    # check that columns in RHISS data contain expected values according to metadata\r\n",
        "    valid_tide <- c(\"L\", \"M\", \"H\")\r\n",
        "    check_tide <- data_df$`Tide` %in% valid_tide\r\n",
        "    check_tide <- ifelse(is.na(check_tide), TRUE, check_tide)\r\n",
        "    \r\n",
        "    cols <- c(\"Slime Height (cm)\", \"Entangled/Mat-Like Height (cm)\", \"Filamentous Height (cm)\", \"Leafy/Fleshy Height (cm)\", \"Tree/Bush-Like Height (cm)\")\r\n",
        "    valid_macroalgae <- c(\">25cm\", \"1 to 3cm\", \"None\", \"4 to 25cm\")\r\n",
        "    available_cols <- cols %in% colnames(data_df)\r\n",
        "    if (sum(available_cols) >= 2){\r\n",
        "      cols <- cols[available_cols]\r\n",
        "      is_valid_macroalgae <- apply(data_df[, cols], 2, function(x) !(x %in% valid_macroalgae))\r\n",
        "      is_valid_macroalgae <- ifelse(is.na(is_valid_macroalgae), FALSE, is_valid_macroalgae)\r\n",
        "      check_macroalgae <- rowSums(is_valid_macroalgae) > 0\r\n",
        "    } else if (sum(available_cols) == 1) {\r\n",
        "      cols <- cols[available_cols]\r\n",
        "      is_valid_macroalgae <- !(data_df[,cols] %in% valid_macroalgae)\r\n",
        "      is_valid_macroalgae <- ifelse(is.na(is_valid_macroalgae), FALSE, is_valid_macroalgae)\r\n",
        "      check_macroalgae <- rowSums(is_valid_macroalgae) > 0\r\n",
        "    } else {\r\n",
        "      check_macroalgae <- 0\r\n",
        "    }\r\n",
        "  \r\n",
        "    \r\n",
        "    valid_descriptive_bleach_severity <- c(\"Totally bleached white\", \"pale/fluoro (very light or yellowish)\", \"None\", \"Bleached only on upper surface\", \"Pale (very light)/Focal bleaching\", \"Totally bleached white/fluoro\", \"Recently dead coral lightly covered in algae\")\r\n",
        "    bcols <- c(\"Mushroom Bleach Severity\", \"Massive Bleach Severity\", \"Encrusting Bleach Severity\", \"Vase/Foliose Bleach Severity\", \"Plate/Table Bleach Severity\", \"Bushy Bleach Severity\", \"Branching Bleach Severity\")\r\n",
        "    available_cols <- bcols %in% colnames(data_df)\r\n",
        "    if (sum(available_cols) >= 2){\r\n",
        "      bcols <- bcols[available_cols]\r\n",
        "      is_valid_descriptive_bleach_severity <- apply(data_df[, bcols], 2, function(x) !(x %in% valid_descriptive_bleach_severity))\r\n",
        "      is_valid_descriptive_bleach_severity <- ifelse(is.na(is_valid_descriptive_bleach_severity), FALSE, is_valid_descriptive_bleach_severity)\r\n",
        "      check_descriptive_bleach_severity <- rowSums(is_valid_macroalgae) > 0\r\n",
        "    } else if (sum(available_cols) == 1) {\r\n",
        "      bcols <- bcols[available_cols]\r\n",
        "      is_valid_descriptive_bleach_severity <- !(data_df[,bcols] %in% valid_descriptive_bleach_severity)\r\n",
        "      is_valid_descriptive_bleach_severity <- ifelse(is.na(is_valid_descriptive_bleach_severity), FALSE, is_valid_descriptive_bleach_severity)\r\n",
        "      check_descriptive_bleach_severity <- rowSums(is_valid_descriptive_bleach_severity) > 0\r\n",
        "    } else {\r\n",
        "      check_descriptive_bleach_severity <- 0\r\n",
        "    }\r\n",
        "    \r\n",
        "    if (all(c(\"Bleached Average Severity Index (calculated via matrix)\") %in% colnames(data_df))){\r\n",
        "      bleached_severity <- data_df$`Bleached Average Severity Index (calculated via matrix)`\r\n",
        "      check_bleach_severity <- bleached_severity >= 1 & bleached_severity <= 8\r\n",
        "      check_bleach_severity <- ifelse(is.na(check_bleach_severity), TRUE, check_bleach_severity)\r\n",
        "    } else {\r\n",
        "      check_bleach_severity <- 1\r\n",
        "    }\r\n",
        "  \r\n",
        "    \r\n",
        "    check <- !check_tide | check_macroalgae | !check_bleach_severity | check_descriptive_bleach_severity\r\n",
        "    if (any(check)) {\r\n",
        "      grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "      parent <- as.character(match.call())[1]\r\n",
        "      if (any(!check_tide)) {\r\n",
        "        warning1 <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have invalid tide values:\",\r\n",
        "                          toString(data_df[!check_tide , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[!check_tide]))\r\n",
        "        \r\n",
        "      }\r\n",
        "      if (any(check_macroalgae)) {\r\n",
        "        warning2 <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have invalid macroalgae values:\",\r\n",
        "                          toString(data_df[check_macroalgae, 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[check_macroalgae]))\r\n",
        "      }\r\n",
        "      if (any(!check_bleach_severity)) {\r\n",
        "        warning3 <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have invalid bleach severity values:\",\r\n",
        "                          toString(data_df[!check_bleach_severity, 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[!check_bleach_severity]))\r\n",
        "      }\r\n",
        "      if (any(check_descriptive_bleach_severity)) {\r\n",
        "        warning4 <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have invalid descriptive beach severity:\",\r\n",
        "                          toString(data_df[check_descriptive_bleach_severity , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[check_descriptive_bleach_severity ]))\r\n",
        "      }\r\n",
        "      \r\n",
        "      if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "        # Append the warning to an existing matrix \r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[!check_tide, 1],\r\n",
        "          index = which(!check_tide),\r\n",
        "          message = \"Invalid tide value\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Tide\", warnings, parent_key = \"Warning\")\r\n",
        "        \r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[check_macroalgae, 1],\r\n",
        "          index = which(check_macroalgae),\r\n",
        "          message = \"Invalid macroalgae\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Macroalgae\", warnings, parent_key = \"Warning\")\r\n",
        "        \r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[!check_bleach_severity, 1],\r\n",
        "          index = which(!check_bleach_severity),\r\n",
        "          message = \"Invalid Bleach Severity\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Bleach Severity\", warnings, parent_key = \"Warning\")\r\n",
        "        \r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[check_descriptive_bleach_severity, 1],\r\n",
        "          index = which(check_descriptive_bleach_severity),\r\n",
        "          message = \"Invalid Bleach Severity Description\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Bleach Severity Description\", warnings, parent_key = \"Warning\")\r\n",
        "      }\r\n",
        "      \r\n",
        "    }\r\n",
        "    data_df[[\"error_flag\"]] <- as.integer(data_df[[\"error_flag\"]] | check)\r\n",
        "    \r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_RHISS\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_RHISS\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "update_path <- function(path) {\r\n",
        "  # Replace backslashes with forward slashes\r\n",
        "  fixed_path <- gsub(\"\\\\\\\\\", \"/\", path)\r\n",
        "  # Reduce multiple forward slashes to a single one\r\n",
        "  normalized_path <- gsub(\"/{2,}\", \"/\", fixed_path)\r\n",
        "  return(normalized_path)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "verify_percentages <- function(data_df) {\r\n",
        "  tryCatch({\r\n",
        "    # Check that all percentages in a row are between 0 and 100\r\n",
        "    perc_cols <- grep(\"%\", colnames(data_df))\r\n",
        "    if(length(perc_cols) > 0){\r\n",
        "      perc_cols_vals <- data_df[, perc_cols]\r\n",
        "      col_check <- apply(perc_cols_vals, 2, function(x) x < 0 | x > 100)\r\n",
        "      col_check <- ifelse(is.na(col_check), FALSE, col_check)\r\n",
        "      check <- rowSums(col_check) > 0\r\n",
        "      data_df[[\"error_flag\"]] <- as.integer(data_df[[\"error_flag\"]] | check)\r\n",
        "      if (any(check)) {\r\n",
        "        grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "        parent <- as.character(match.call())[1]\r\n",
        "        warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have percentages in an invalid format:\",\r\n",
        "                         toString(data_df[check , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[check]))\r\n",
        "        base::message(warning)\r\n",
        "        \r\n",
        "        if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "          # Append the warning to an existing matrix \r\n",
        "          warnings <- data.frame(\r\n",
        "            ID = data_df[check, 1],\r\n",
        "            index = which(check),\r\n",
        "            message = \"Invalid Percentages\"\r\n",
        "          )\r\n",
        "          contribute_to_metadata_report(\"Percentages\", warnings, parent_key = \"Warning\")\r\n",
        "          \r\n",
        "        }\r\n",
        "        \r\n",
        "      }\r\n",
        "    }\r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_percentages\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_percentages\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "verify_na_null <- function(data_df, configuration) {\r\n",
        "  tryCatch({\r\n",
        "    # check if there are any values that are NA or NULL and flag those rows as an \r\n",
        "    # error. This does not include new additional columns as they are assigned a\r\n",
        "    # default value at the end of the verification process or ID column.\r\n",
        "\r\n",
        "    transformations <- configuration$mappings$transformations\r\n",
        "    new_fields <- configuration$mappings$new_fields\r\n",
        "    \r\n",
        "    nonexempt_existing_cols <- transformations[transformations$verify_na_exempt == FALSE, \"target_field\"]\r\n",
        "    nonexempt_new_cols <- new_fields[new_fields$verify_na_exempt == FALSE, \"field\"]\r\n",
        "    nonexempt_cols <- c(nonexempt_new_cols, nonexempt_existing_cols)\r\n",
        "    nonexempt_cols <- nonexempt_cols[nonexempt_cols %in% colnames(data_df)]\r\n",
        "    nonexempt_df <- data_df[,nonexempt_cols]\r\n",
        "    na_present <- apply(nonexempt_df, 2, function(x) is.na(x) | is.null(x) | x == \"\")\r\n",
        "    na_present <- ifelse(is.na(na_present), TRUE, na_present)\r\n",
        "    check <- rowSums(na_present) > 0\r\n",
        "    data_df[,\"error_flag\"] <- as.integer(data_df[,\"error_flag\"] | check)\r\n",
        "    if (any(check)) {\r\n",
        "      grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "      parent <- as.character(match.call())[1]\r\n",
        "      warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have missing data:\",\r\n",
        "                       toString(data_df[check , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[check]))\r\n",
        "      base::message(warning)\r\n",
        "      \r\n",
        "      if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "        # Append the warning to an existing matrix \r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[check, 1],\r\n",
        "          index = which(check),\r\n",
        "          message = \"NA or Null values present\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"NA|NULL\", warnings, parent_key = \"Warning\")\r\n",
        "        \r\n",
        "      }\r\n",
        "      \r\n",
        "    }\r\n",
        "    return(data_df)\r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_na_null\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_na_null\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "verify_integers_positive <- function(data_df) {\r\n",
        "  tryCatch({\r\n",
        "    # R function that verifys all integers are positive values as they represent \r\n",
        "    # real quantities. Note: Whole numbers are not integers, they must be declared\r\n",
        "    # as such. All relevant columns were set as integers in the set_data_type \r\n",
        "    # function\r\n",
        "    \r\n",
        "    is_integer <- sapply(data_df[1,],is.integer)\r\n",
        "    if(any(is_integer)){\r\n",
        "      check <- apply(data_df[,is_integer, drop = FALSE], 1, function(row) any(row < 0))\r\n",
        "      check <- ifelse(is.na(check), FALSE, check)\r\n",
        "      data_df[, \"error_flag\"] <- as.integer(data_df[, \"error_flag\"] | check)\r\n",
        "      \r\n",
        "      if (any(check)) {\r\n",
        "        grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "        parent <- as.character(match.call())[1]\r\n",
        "        warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have non-positive integer values:\",\r\n",
        "                         toString(data_df[check , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[check]))\r\n",
        "        base::message(warning)\r\n",
        "        \r\n",
        "        \r\n",
        "        if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "          # Append the warning to an existing matrix \r\n",
        "          warnings <- data.frame(\r\n",
        "            ID = data_df[check, 1],\r\n",
        "            index = which(check),\r\n",
        "            message = \"Non-positive integers present\"\r\n",
        "          )\r\n",
        "          contribute_to_metadata_report(\"Integers\", warnings, parent_key = \"Warning\")\r\n",
        "          \r\n",
        "        }\r\n",
        "      }\r\n",
        "    }\r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_integers_positive\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_integers_positive\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "remove_leading_spaces <- function(data_df) {\r\n",
        "  # R function that removes leading and trailing spaces from all entries in a \r\n",
        "  # data frame column\r\n",
        "  cols <- colnames(data_df)\r\n",
        "  for (col_name in cols) {\r\n",
        "    is_character <- is.character(data_df[[col_name]])\r\n",
        "    if(any(is_character)){\r\n",
        "      data_df[is_character, col_name] <- gsub(\"^\\\\s+|\\\\s+$\", \"\", data_df[is_character, col_name])\r\n",
        "    } \r\n",
        "  }\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "verify_coral_cover <- function(data_df) {\r\n",
        "  tryCatch({\r\n",
        "    accepted_values <- c(\"1-\", \"2-\", \"3-\", \"4-\", \"5-\", \"1+\", \"2+\", \"3+\", \"4+\", \"5+\", \"0\")\r\n",
        "    \r\n",
        "    # Identify possible corrections based on common mistakes\r\n",
        "    possible_corrections <- data.frame(\r\n",
        "      Old_Value = c(\"-1\", \"-2\", \"-3\", \"-4\", \"-5\",\"+1\", \"+2\", \"+3\", \"+4\", \"+5\"),\r\n",
        "      New_Value = c(\"1-\", \"2-\", \"3-\", \"4-\", \"5-\", \"1+\", \"2+\", \"3+\", \"4+\", \"5+\")\r\n",
        "    )\r\n",
        "    \r\n",
        "    col_names <- colnames(data_df)\r\n",
        "    \r\n",
        "    # Convert column names to lowercase\r\n",
        "    col_names <- tolower(col_names)\r\n",
        "    \r\n",
        "    # Iterate through the column names containing \"coral\"\r\n",
        "    for (col in col_names) {\r\n",
        "      if (grepl(\"coral\", col)) {\r\n",
        "        # Substitute similar characters for correct ones\r\n",
        "        data_df[[col]] <- gsub(\"||\", \"-\", data_df[[col]])\r\n",
        "       \r\n",
        "        # Loop through possible corrections and replace values\r\n",
        "        for (i in 1:nrow(possible_corrections)) {\r\n",
        "          old_value <- possible_corrections$Old_Value[i]\r\n",
        "          new_value <- possible_corrections$New_Value[i]\r\n",
        "          data_df[[col]][data_df[[col]] == old_value] <- new_value\r\n",
        "        }\r\n",
        "        \r\n",
        "        check <- data_df[[col]] %in% accepted_values\r\n",
        "        check <- ifelse(is.na(check), TRUE, check)\r\n",
        "        check <- !check\r\n",
        "        \r\n",
        "        data_df[[\"error_flag\"]] <- as.integer(data_df[[\"error_flag\"]] | check)\r\n",
        "        \r\n",
        "        if (any(check)) {\r\n",
        "          grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "          parent <- as.character(match.call())[1]\r\n",
        "          warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have invalid coral cover values:\",\r\n",
        "                           toString(data_df[check , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[check]))\r\n",
        "          base::message(warning)\r\n",
        "          \r\n",
        "          \r\n",
        "          if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "            # Append the warning to an existing matrix \r\n",
        "            warnings <- data.frame(\r\n",
        "              ID = data_df[check, 1],\r\n",
        "              index = which(check),\r\n",
        "              message = \"Invalid coral cover values\"\r\n",
        "            )\r\n",
        "            contribute_to_metadata_report(col, warnings, parent_key = \"Warning\")\r\n",
        "          }\r\n",
        "        }\r\n",
        "      }\r\n",
        "    }\r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_coral_cover\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_coral_cover\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "verify_reef <- function(data_df){\r\n",
        "  tryCatch({\r\n",
        "    # Check that the reef ID is in one of the correct standard formats with regex.\r\n",
        "    # Look for most similar reef ID if it is not. Am not checking for a match \r\n",
        "    # because that would mean no new reefs would be accepted and I believe that \r\n",
        "    # the reef input is restricted to existing reefs so it is unlikley to be a typo\r\n",
        "    reef_id_col_to_find <- c(\"reef id\", \"reef label\", \"reef_id\", \"reef_label\")\r\n",
        "    col_names <- colnames(data_df)\r\n",
        "    col_names_lower <- tolower(col_names)\r\n",
        "    is_col_present <- sapply(reef_id_col_to_find, function(string) grepl(string, col_names_lower))\r\n",
        "    reef_id_col_index <- which(rowSums(is_col_present) > 0)\r\n",
        "    reef_id_cols <- col_names[reef_id_col_index]\r\n",
        "    for (reef_id_col in reef_id_cols){\r\n",
        "      reef_id <- data_df[[reef_id_col]]\r\n",
        "      correct_reef_id_format <- grepl(\"^(1[0-9]|2[0-9]|10)-\\\\d{3}[a-z]?$\", reef_id)\r\n",
        "      data_df[,\"error_flag\"] <- as.integer(data_df[,\"error_flag\"] | !correct_reef_id_format)\r\n",
        "      if (any(!correct_reef_id_format)) {\r\n",
        "        grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "        parent <- as.character(match.call())[1]\r\n",
        "        warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have invalid Reef IDs:\",\r\n",
        "                         toString(data_df[!correct_reef_id_format , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[!correct_reef_id_format]))\r\n",
        "        base::message(warning)\r\n",
        "        \r\n",
        "        if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "          # Append the warning to an existing matrix \r\n",
        "          warnings <- data.frame(\r\n",
        "            ID = data_df[!correct_reef_id_format, 1],\r\n",
        "            index = which(!correct_reef_id_format),\r\n",
        "            message = \"Invalid reef ID\"\r\n",
        "          )\r\n",
        "          contribute_to_metadata_report(reef_id_col, warnings, parent_key = \"Warning\")\r\n",
        "        }\r\n",
        "      }\r\n",
        "    }\r\n",
        "    \r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_reef\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_reef\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "verify_voyage_dates <- function(data_df){\r\n",
        "  tryCatch({\r\n",
        "    # Check that voyage dates of observation are within in voyage dates and that \r\n",
        "    # none of the dates are NA. If Voyage dates are NA set start and end to min \r\n",
        "    # and max observation date. Check that voyage dates associated with a vessels \r\n",
        "    # voyage are unique (There should only be on departure and return date)\r\n",
        "    voyage_start <- data_df[[\"Voyage Start\"]]\r\n",
        "    voyage_end <- data_df[[\"Voyage End\"]]\r\n",
        "    \r\n",
        "    incomplete_dates <- unique(c(which(is.na(voyage_end)), which(is.na(voyage_start))))\r\n",
        "    if (length(incomplete_dates) > 0){\r\n",
        "      incomplete_date_rows <- data_df[incomplete_dates,]\r\n",
        "      vessel_voyage <- unique(incomplete_date_rows[,which(names(incomplete_date_rows) %in% c(\"Vessel\", \"Voyage\"))])\r\n",
        "      for(i in 1:nrow(vessel_voyage)){\r\n",
        "        data_df_filtered <- data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]),]\r\n",
        "        is_any_voyage_date_correct <- any(!is.na(data_df_filtered$`Voyage Start`) & !is.na(data_df_filtered$`Voyage End`))\r\n",
        "        is_any_survey_date_correct <- any(!is.na(data_df_filtered$`Survey Date`))\r\n",
        "        if(is_any_voyage_date_correct){\r\n",
        "          correct_dates <- data_df_filtered[!is.na(data_df_filtered),][1,c(\"Voyage Start\", \"Voyage End\")]\r\n",
        "          data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]) & (is.na(data_df$`Voyage Start`)), \"Voyage Start\"] <- correct_dates[1]\r\n",
        "          data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]) & (is.na(data_df$`Voyage End`)), \"Voyage End\"] <- correct_dates[2]\r\n",
        "        } else if(!is_any_voyage_date_correct & is_any_survey_date_correct){\r\n",
        "          incomplete_date_rows_filtered <- incomplete_date_rows[(incomplete_date_rows$Vessel == vessel_voyage[i,1]) & (incomplete_date_rows$Voyage == vessel_voyage[i,2]),]\r\n",
        "          estimate_start <- min(incomplete_date_rows_filtered$`Survey Date`)\r\n",
        "          estimate_end <- min(incomplete_date_rows_filtered$`Survey Date`)\r\n",
        "          data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]) & (is.na(data_df$`Voyage Start`)), c(\"Voyage Start\")] <-  estimate_start\r\n",
        "          data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]) & (is.na(data_df$`Voyage End`)), c(\"Voyage End\")] <-  estimate_end\r\n",
        "        } else if(!is_any_voyage_date_correct & !is_any_survey_date_correct) {\r\n",
        "          data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]) & (is.na(data_df$`Voyage Start`) | is.na(data_df$`Voyage End`)), \"error_flag\"] <- 1\r\n",
        "        }\r\n",
        "      }\r\n",
        "      \r\n",
        "    }\r\n",
        "    \r\n",
        "    \r\n",
        "    post_voyage_start <- data_df[[\"Voyage Start\"]]\r\n",
        "    post_voyage_end <- data_df[[\"Voyage End\"]]\r\n",
        "    na_present <- ((is.na(post_voyage_start) | is.null(post_voyage_start)) | (is.na(post_voyage_end) | is.null(post_voyage_end))) & ((is.na(voyage_start) | is.null(voyage_start)) | (is.na(voyage_end) | is.null(voyage_end)))\r\n",
        "    dated_estimated <- !((is.na(post_voyage_start) | is.null(post_voyage_start)) | (is.na(post_voyage_end) | is.null(post_voyage_end))) & ((is.na(voyage_start) | is.null(voyage_start)) | (is.na(voyage_end) | is.null(voyage_end)))\r\n",
        "    \r\n",
        "    if (any(dated_estimated | na_present)) {\r\n",
        "      grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "      parent <- as.character(match.call())[1]\r\n",
        "      if (any(dated_estimated) & !any(na_present)) {\r\n",
        "        warning1 <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have their voyage dates estimated from their vessel\",\r\n",
        "                          toString(data_df[dated_estimated , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[dated_estimated]))\r\n",
        "        base::message(warning1)\r\n",
        "      }\r\n",
        "      if (any(na_present)) {\r\n",
        "        warning2 <- paste(\"Warning in\", parent , \"within\", grandparent, \"- The rows with the following IDs have voyage dates\",\r\n",
        "                          toString(data_df[na_present , 1]), \"Their respective row indexes are:\", toString((1:nrow(data_df))[na_present]))\r\n",
        "        base::message(warning2)\r\n",
        "      }  \r\n",
        "      \r\n",
        "      if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "        # Append the warning to an existing matrix \r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[dated_estimated, 1],\r\n",
        "          index = which(dated_estimated),\r\n",
        "          message = \"Invalid Voyage Date. Date was successfully estimated.\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Estimated Voyage Date\", warnings, parent_key = \"Warning\")\r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[na_present, 1],\r\n",
        "          index = which(na_present),\r\n",
        "          message = \"Invalid Voyage Date. \"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Invalid Voyage Date\", warnings, parent_key = \"Warning\")\r\n",
        "      }\r\n",
        "      \r\n",
        "    }\r\n",
        "    \r\n",
        "    survey_date_error <- (data_df$`Survey Date` < data_df$`Voyage Start` | data_df$`Survey Date` > data_df$`Voyage End`)\r\n",
        "    data_df$error_flag <- data_df$error_flag | survey_date_error\r\n",
        "    \r\n",
        "    if(any(survey_date_error)){\r\n",
        "      if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "        # Append the warning to an existing matrix \r\n",
        "        warnings <- data.frame(\r\n",
        "          ID = data_df[survey_date_error, 1],\r\n",
        "          index = which(survey_date_error),\r\n",
        "          message = \"Survey date outside voyage date range\"\r\n",
        "        )\r\n",
        "        contribute_to_metadata_report(\"Survey Date\", warnings, parent_key = \"Warning\")\r\n",
        "      }\r\n",
        "    }\r\n",
        "    \r\n",
        "    vessel_voyage <- unique(data_df[,which(names(data_df) %in% c(\"Vessel\", \"Voyage\"))])\r\n",
        "    for (i in 1:nrow(vessel_voyage)){\r\n",
        "      filtered_data_df <- data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]),]\r\n",
        "      start_dates <- unique(filtered_data_df$`Voyage Start`) \r\n",
        "      end_dates <- unique(filtered_data_df$`Voyage End`)\r\n",
        "      if(length(start_dates) > 1){\r\n",
        "        mf_start <- names(sort(table(start_dates), decreasing = TRUE)[1])\r\n",
        "        data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]), \"Voyage Start\"] <- mf_start\r\n",
        "      }\r\n",
        "      if(length(end_dates) > 1){\r\n",
        "        mf_end <- names(sort(table(end_dates), decreasing = TRUE)[1])\r\n",
        "        data_df[(data_df$Vessel == vessel_voyage[i,1]) & (data_df$Voyage == vessel_voyage[i,2]), \"Voyage End\"] <- mf_end\r\n",
        "      }\r\n",
        "    }\r\n",
        "  }, error = function(e){\r\n",
        "    errors <- data.frame(\r\n",
        "      verification_function = \"verify_voyage_dates\",\r\n",
        "      message = as.character(e)\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"verify_voyage_dates\", errors, parent_key = \"Error In Verification\")\r\n",
        "    data_df$error_flag <<- 1\r\n",
        "  })\r\n",
        "  return(data_df)\r\n",
        "}\r\n",
        "\r\n",
        "find_one_to_one_matches <- function(close_match_rows){\r\n",
        "  \r\n",
        "  x_df_col <- 1\r\n",
        "  y_df_col <- 2\r\n",
        "  \r\n",
        "  # Determine duplicates of the row indices.\r\n",
        "  x_dup_indices <- (duplicated(close_match_rows[,x_df_col])|duplicated(close_match_rows[,x_df_col], fromLast=TRUE))\r\n",
        "  y_dup_indices <- (duplicated(close_match_rows[,y_df_col])|duplicated(close_match_rows[,y_df_col], fromLast=TRUE))\r\n",
        "  dup_indices <- y_dup_indices|x_dup_indices\r\n",
        "  non_dup_indices <- !dup_indices\r\n",
        "  \r\n",
        "  # Add rows matches with only a single to relevant matrix.\r\n",
        "  perfect_duplicate_indices <<- rbind(perfect_duplicate_indices, close_match_rows[non_dup_indices & close_match_rows[,3] == 0,])\r\n",
        "  discrepancies_indices <<- rbind(discrepancies_indices, close_match_rows[non_dup_indices & close_match_rows[,3] > 0,])\r\n",
        "  \r\n",
        "  # remove rows that have already been handled to prevent double handling.\r\n",
        "  close_match_rows_updated <- close_match_rows[dup_indices,]\r\n",
        "  return(close_match_rows_updated)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "rec_group <- function(m2m_split, groups, group){\r\n",
        "  # That is recursive with the purpose of grouping sets of matching rows so that \r\n",
        "  # it can be determined whether or not they are mistakes or coincidental \r\n",
        "  # perfect matches.  \r\n",
        "  group <- group + 1\r\n",
        "  stack <- m2m_split[[group]][,1]\r\n",
        "  names <- names(m2m_split)\r\n",
        "  for(i in 1:length(names)){\r\n",
        "    if(identical(stack,m2m_split[[names[i]]][,1])){\r\n",
        "      groups[[group]] <- c(groups[[group]], m2m_split[[names[i]]][1,2])\r\n",
        "    }\r\n",
        "    \r\n",
        "  }\r\n",
        "  if(length(m2m_split) != group){\r\n",
        "    return(rec_group(m2m_split, groups, group))\r\n",
        "  } else {\r\n",
        "    return(groups)\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "# Re-write base operator %in% faster\r\n",
        "`%fin%` <- function(x, table) {\r\n",
        "  stopifnot(require(fastmatch))\r\n",
        "  fmatch(x, table, nomatch = 0L) > 0L\r\n",
        "}\r\n",
        "\r\n",
        "matrix_close_matches_vectorised <- function(x, y, distance){\r\n",
        "  # Find list of all close matches between rows in x and y within a specified \r\n",
        "  # distance. This distance is the number of non perfect column matches within\r\n",
        "  # a row. returns a the indices of the rows matched and the distance from\r\n",
        "  # perfect. (X_index, Y_index, Distance). Pre-allocating memory for the matrix\r\n",
        "  # is not definitive and needs to assume worst case scenario or maximum \r\n",
        "  # allocation possible. However, this requires to much memory, and therefore \r\n",
        "  # will dynamically allocate memory as needed even though this is slower.\r\n",
        "  \r\n",
        "  #Pre-allocate variables and memory \r\n",
        "  x_rows <- nrow(x)\r\n",
        "  x_cols <- ncol(x)\r\n",
        "  y_rows <- nrow(y)\r\n",
        "  \r\n",
        "  match_indices <- matrix(nrow = 0, ncol = 3)\r\n",
        "  \r\n",
        "  # Iterate through each row in matrix/dataframe x and evaluate for each value \r\n",
        "  # in the row whether it matches the column of y. This vector of logical values\r\n",
        "  # are then appeneded to the `matches` matrix. After Iterating over every \r\n",
        "  # column there will be a matrix of size (y_rows, x_cols). A perfect matching \r\n",
        "  # row in y will have a corresponding row in `matches` exclusively containing \r\n",
        "  # TRUE. Given TRUE = 1, a row sum can be utilised to determine the distance \r\n",
        "  # from a perfect match. Can then use a customised vectorised function to \r\n",
        "  # append matches to match_indices. \r\n",
        "  matches <- matrix(data=NA, nrow=y_rows, ncol=x_cols)\r\n",
        "  for(z in 1:x_rows){ \r\n",
        "    for(i in 1:x_cols){\r\n",
        "      matches[,i] <- x[z,i] == y[,i]\r\n",
        "    }\r\n",
        "    matches <- !(matches)\r\n",
        "    num_matches <- rowSums(matches, dims = 1)\r\n",
        "    num_matches <- ifelse(num_matches <= distance, num_matches, NA)\r\n",
        "    nonNA <- which(!is.na(num_matches))\r\n",
        "    nonNAvalues <- num_matches[nonNA]\r\n",
        "    if(length(nonNAvalues) >= 1){\r\n",
        "      match <- store_index_vec(nonNAvalues, nonNA, z)\r\n",
        "      match <- t(match)\r\n",
        "      match_indices <- rbind(match_indices, match)\r\n",
        "    }\r\n",
        "  }\r\n",
        "  match_indices <- na.omit(match_indices)\r\n",
        "  return(match_indices)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "store_index <- function(nonNAvalues, nonNA, z){\r\n",
        "  match_indices <- c(z, nonNA, nonNAvalues)\r\n",
        "  return(match_indices)\r\n",
        "}\r\n",
        "store_index_vec <- base::Vectorize(store_index)\r\n",
        "\r\n",
        "set_data_type <- function(data_df, mapping){\r\n",
        "  # sets the data_type of each column of any data frame input based on\r\n",
        "  # configuration file\r\n",
        "  \r\n",
        "  output_df <- data_df\r\n",
        "  for (i in seq_len(nrow(mapping))) {\r\n",
        "    column_name <- mapping$field[i]\r\n",
        "    data_type <- mapping$data_type[i]\r\n",
        "    if (!(column_name %in% colnames(data_df))){\r\n",
        "      next\r\n",
        "    }\r\n",
        "    # Convert the column to the specified data type\r\n",
        "    if(tolower(data_type) == \"date\"){\r\n",
        "      # NA will cause all dates to note parse. Remove NA from parsing but \r\n",
        "      # then include them in the dataframe afterwards. \r\n",
        "      datetimes <- data_df[[column_name]]\r\n",
        "      datetimes_available <- datetimes[!is.na(datetimes)]\r\n",
        "      dates_available <- parse_date_time(datetimes_available, orders = get_datetime_parse_order())\r\n",
        "      dates_available <- as.character(date(dates_available))\r\n",
        "      datetimes[!is.na(datetimes)] <- dates_available\r\n",
        "      output_df[[column_name]] <- datetimes\r\n",
        "    } else if (tolower(data_type) == \"time\") {\r\n",
        "      time <- as.POSIXct(data_df[[column_name]], format = \"%H:%M:%S\")\r\n",
        "      output_df[[column_name]] <- format(time, '%H:%M:%S')\r\n",
        "    } else if (tolower(data_type) == \"datetime\") {\r\n",
        "      output_df[[column_name]] <- as.character(parse_date_time(data_df[[column_name]], orders = get_datetime_parse_order()))\r\n",
        "    } else {\r\n",
        "      output_df[[column_name]] <- as(data_df[[column_name]], tolower(data_type))\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  # Identify which rows have been coerced to NA and listed in metadata report. \r\n",
        "  original_has_na <- apply(data_df, 2, function(x) is.na(x))\r\n",
        "  conversion_has_na <- apply(output_df, 2, function(x) is.na(x))\r\n",
        "  coerced_na <- rowSums(original_has_na) < rowSums(conversion_has_na)\r\n",
        "  \r\n",
        "  if(any(coerced_na)){\r\n",
        "    \r\n",
        "    grandparent <- as.character(sys.call(sys.parent()))[1]\r\n",
        "    parent <- as.character(match.call())[1]\r\n",
        "    indexes <- (1:length(coerced_na))\r\n",
        "    warning <- paste(\"Warning in\", parent , \"within\", grandparent, \"- Incorrect data type present. The rows with the following IDs have had value(s) coerced to NA:\",\r\n",
        "                     paste(data_df[coerced_na, 1], collapse = \", \"), \"and the following indexes\", paste(indexes[coerced_na], collapse = \", \"))\r\n",
        "    base::message(warning)\r\n",
        "    \r\n",
        "    if (exists(\"contribute_to_metadata_report\") && is.function(contribute_to_metadata_report)) {\r\n",
        "      # Append the warning to an existing matrix \r\n",
        "      warnings <- data.frame(\r\n",
        "        ID = data_df[coerced_na, 1],\r\n",
        "        index = which(coerced_na),\r\n",
        "        message = \"Incorrect data type present. Values coerced to NA\"\r\n",
        "      )\r\n",
        "      contribute_to_metadata_report(\"Data Type\", warnings, parent_key = \"Warning\")\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  # remove trailing and leading spaces from strings for comparison. \r\n",
        "  output_df <- remove_leading_spaces(output_df)\r\n",
        "  return(output_df)\r\n",
        "}\r\n",
        "\r\n",
        "update_config_file <- function(data_df, configuration_path, new_mappings_to_add=c()) {\r\n",
        "  \r\n",
        "  configuration <- fromJSON(configuration_path)\r\n",
        "  data_colnames <- colnames(data_df)\r\n",
        "  expected_source_names <- configuration$mappings$transformations$source_field\r\n",
        "  new_json_data <- configuration\r\n",
        "  \r\n",
        "  if (!all(data_colnames %in% expected_source_names)) {\r\n",
        "    warning <- \"Column names in 'data_df' do not match the expected source names. New json configuration file will be created with most appropriate mapping. Please check after process is complete.\"\r\n",
        "    warning(warning)\r\n",
        "    closest_matches <- get_closest_matches(data_colnames, expected_source_names)\r\n",
        "    # Replace the original source values in new_json_data with closest matches\r\n",
        "    \r\n",
        "    old_values <- c()\r\n",
        "    new_values <- c()\r\n",
        "    for (i in seq_len(ncol(closest_matches))) {\r\n",
        "      new_input <- closest_matches[1, i]\r\n",
        "      closest_match <- closest_matches[2, i]\r\n",
        "      index <- which(new_json_data$mappings$transformations$source_field %in% closest_match)\r\n",
        "      if (!is.na(index) ) {\r\n",
        "        new_json_data$mappings$transformations$source_field[index] <- new_input\r\n",
        "        old_values <- c(old_values, closest_matches)\r\n",
        "        new_values <- c(new_values, new_input)\r\n",
        "      }\r\n",
        "    }\r\n",
        "    warnings <- data.frame(\r\n",
        "      old_values = old_values,\r\n",
        "      new_values = new_values,\r\n",
        "      message = \"Mapping has been updated\"\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"configuration Mapping Update\", warnings, parent_key = \"Warning\")\r\n",
        "  }\r\n",
        "  \r\n",
        "  if(length(new_mappings_to_add) > 0){\r\n",
        "    max_position <- base::max(configuration$mappings$transformations$position, configuration$mappings$new_fields$position)\r\n",
        "    closest_matches <- get_closest_matches(new_mappings_to_add, data_colnames)\r\n",
        "    contribute_to_metadata_report(\"New configuration Mappings\", paste(closest_match, \"updated to\", new_input), parent_key = \"Warning\")\r\n",
        "    for(i in 1:length(new_mappings_to_add)){\r\n",
        "      source_field <- closest_matches[2,i]\r\n",
        "      target_field <- closest_matches[2,i]\r\n",
        "      position <- max_position + i\r\n",
        "    }\r\n",
        "    warnings <- data.frame(\r\n",
        "      field_name = closest_matches[2,i],\r\n",
        "      message = \"Mappings have been created for the following field names\"\r\n",
        "    )\r\n",
        "    contribute_to_metadata_report(\"configuration Mapping Update\", warnings, parent_key = \"Warning\")\r\n",
        "  }\r\n",
        "  \r\n",
        "  if (!all(data_colnames %in% expected_source_names) || (length(new_mappings_to_add) > 0)) {\r\n",
        "    json_data <- toJSON(new_json_data, pretty = TRUE)\r\n",
        "    directory <- dirname(configuration_path)\r\n",
        "    tryCatch({\r\n",
        "      writeLines(json_data, file.path(directory, paste(configuration$metadata$control_data_type, \"_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".json\", sep = \"\")))\r\n",
        "    }, error = function(e){\r\n",
        "      writeLines(json_data, file.path(paste(configuration$metadata$control_data_type, \"_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".json\", sep = \"\")))\r\n",
        "    })\r\n",
        "    \r\n",
        "  }\r\n",
        "  \r\n",
        "  if(length(new_mappings_to_add) > 0){\r\n",
        "    tryCatch({\r\n",
        "      configuration <- fromJSON(file.path(directory, paste(configuration$metadata$control_data_type, \"_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".json\", sep = \"\")))\r\n",
        "    }, error = function(e){\r\n",
        "      tryCatch({\r\n",
        "        configuration <<- fromJSON(file.path(paste(configuration$metadata$control_data_type, \"_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".json\", sep = \"\")))\r\n",
        "      })\r\n",
        "    })\r\n",
        "  }\r\n",
        "  \r\n",
        "  return(configuration)\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "map_new_fields <- function(data_df, transformed_df, new_fields){\r\n",
        "  for (i in seq_len(nrow(new_fields))) {\r\n",
        "    new_field <- new_fields$field[i]\r\n",
        "    default_value <- new_fields$default[i]\r\n",
        "    \r\n",
        "    # Call function written as string in configuration file\r\n",
        "    if(new_fields$function_call[i]){\r\n",
        "      default_value <- NA\r\n",
        "    }\r\n",
        "    \r\n",
        "    position <- new_fields$position[i]\r\n",
        "    transformed_df[, position] <- default_value\r\n",
        "    colnames(transformed_df)[position] <- new_field\r\n",
        "  }\r\n",
        "  return(transformed_df)\r\n",
        "}  \r\n",
        "\r\n",
        "# Get the default value of new columns required in the dataframe from the\r\n",
        "# configuration file.The default value can be a function in the form of string \r\n",
        "# that will be executed if the default value is dependent on other columns. \r\n",
        "get_new_field_default_values <- function(new_data_df, transformed_data_df, new_fields){\r\n",
        "  new_fields <- new_fields[new_fields$function_call == TRUE,]\r\n",
        "  for (i in seq_len(nrow(new_fields))) {\r\n",
        "        default_value <- new_fields$default[i]\r\n",
        "        expression <- gsub(\"\\\\{TRANSFORMED_DATAFRAME\\\\}\", \"transformed_data_df\", default_value)\r\n",
        "        expression <- gsub(\"\\\\{RAW_DATAFRAME\\\\}\", \"new_data_df\", expression)\r\n",
        "        default_value <- eval(parse(text = expression))\r\n",
        "        position <- new_fields$position[i]\r\n",
        "        transformed_data_df[, position] <- default_value\r\n",
        "  }\r\n",
        "  return(transformed_data_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "map_all_fields <- function(data_df, transformed_df, mappings){\r\n",
        "  closest_matches <- get_closest_matches(colnames(data_df), mappings$source_field)\r\n",
        "  for (i in seq_len(ncol(closest_matches))) {\r\n",
        "    index <- match(closest_matches[2,i], mappings$source_field)\r\n",
        "    position <- mappings$position[index]\r\n",
        "    mapped_name <- mappings$target_field[index]\r\n",
        "    colnames(transformed_df)[position] <- mapped_name \r\n",
        "    transformed_df[, position] <- data_df[[closest_matches[1,i]]]\r\n",
        "  }\r\n",
        "  return(transformed_df)\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "map_data_structure <- function(data_df, mappings, new_fields){\r\n",
        "  \r\n",
        "  transformed_df <- data.frame(matrix(ncol = nrow(mappings) + nrow(new_fields), nrow = nrow(data_df)))\r\n",
        "  transformed_df <- map_new_fields(data_df, transformed_df, new_fields)\r\n",
        "  transformed_df <- map_all_fields(data_df, transformed_df, mappings)\r\n",
        "  transformed_df <- get_new_field_default_values(data_df, transformed_df, new_fields)\r\n",
        "  transformed_df <- transformed_df[, colSums(is.na(transformed_df)) < nrow(transformed_df)]\r\n",
        "  return(transformed_df)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "get_closest_matches <- function(sources, targets){\r\n",
        "  \r\n",
        "  # perform swap to ensure that sources is longer than targets\r\n",
        "  is_targets_longer <- length(targets) > length(sources)\r\n",
        "  if(is_targets_longer){\r\n",
        "    temp <- targets\r\n",
        "    targets <- sources\r\n",
        "    sources <- temp\r\n",
        "  }  \r\n",
        "  \r\n",
        "  transformed_sources <- matrix(0, nrow = 2, ncol = min(length(sources), length(targets)))\r\n",
        "  levenshtein_distances <- adist(sources , targets)\r\n",
        "  smallest_source_distances <- apply(levenshtein_distances, 1, min)\r\n",
        "  smallest_source_indices <- apply(levenshtein_distances, 2, which.min)\r\n",
        "  transformed_sources[1,] <- sources[smallest_source_indices]\r\n",
        "  transformed_sources[2,] <- targets\r\n",
        "  \r\n",
        "  if(is_targets_longer){\r\n",
        "    transformed_sources[c(1, 2), ] <- transformed_sources[c(2, 1), ]\r\n",
        "  }  \r\n",
        "  return(transformed_sources)\r\n",
        "}\r\n",
        "\r\n",
        "extract_dates <- function(input){\r\n",
        "  input <- sapply(input, basename)\r\n",
        "  dates <- str_extract(input, \"\\\\d{8}_\\\\d{6}\")\r\n",
        "  dates <- ifelse(is.na(dates), str_extract(input, \"\\\\d{4}_\\\\d{1,2}_\\\\d{1,2}_\\\\d{1,2}_\\\\d{1,2}(?:_\\\\d{1,2}(?:_\\\\d{1,2})?)?_[APMapm]{2}\"), dates)\r\n",
        "  dates <- ifelse(is.na(dates), str_extract(input, \"\\\\d{8}\"), dates)\r\n",
        "  dates <- ifelse(is.na(dates), str_extract(input, \"\\\\d{6}\"), dates)\r\n",
        "  date_objects <- parse_date_time(dates, orders = c(\"Ymd_HMS\", \"Y_m_d_H_M_%p\", \"Y_m_d_H_M\", \"Y_m_d_H_M_S_%p\", \"Ymd\", \"ymd\"))\r\n",
        "  return(date_objects)\r\n",
        "}\r\n",
        "\r\n",
        "find_recent_file <- function(directory_path, keyword, file_extension) {\r\n",
        "  # Get a list of files in the directory\r\n",
        "  tryCatch({\r\n",
        "    files <- list.files(file.path(getwd(),directory_path), pattern = paste0(keyword, \".*\\\\.\", file_extension), full.names = TRUE)\r\n",
        "    if (length(files) == 0) {\r\n",
        "      cat(\"No matching files found.\\n\")\r\n",
        "      return(NULL)\r\n",
        "    }\r\n",
        "    file_infos <- lapply(files, file.info)\r\n",
        "    creation_times <- sapply(file_infos, function(info) info$mtime)\r\n",
        "    most_recent_index <- which.max(creation_times)\r\n",
        "    return(files[most_recent_index])\r\n",
        "  }, error = function(e) {\r\n",
        "    print(paste(\"Failed to find recent file: \",  conditionMessage(e)))\r\n",
        "  })\r\n",
        "}\r\n",
        "\r\n",
        "save_spatial_as_raster <- function(output_path, serialized_spatial_path){\r\n",
        "  tryCatch({\r\n",
        "    site_regions <- readRDS(serialized_spatial_path)\r\n",
        "    for(i in 1:length(site_regions)){\r\n",
        "      file_name <- names(site_regions[i])\r\n",
        "      modified_file_name <- gsub(\"/\", \"_\", file_name)\r\n",
        "      if (file.info(output_path)$isdir) {\r\n",
        "        file.remove(output_path)\r\n",
        "        output_path <- dirname(output_path)\r\n",
        "      }\r\n",
        "      writeRaster(site_regions[[i]], filename = file.path(output_path, paste(modified_file_name,\".tif\", sep=\"\"), format = \"GTiff\", overwrite = TRUE))\r\n",
        "    }\r\n",
        "  }, error = function(e) {\r\n",
        "    cat(\"An error occurred while saving\", modified_file_name, \"\\n\")\r\n",
        "  })\r\n",
        "}\r\n",
        "\r\n",
        "get_spatial_differences <- function(kml_data, previous_kml_data){\r\n",
        "  spatial_differences <- list()\r\n",
        "\r\n",
        "    # Extract Reef IDs from each list\r\n",
        "  reef_ids <- get_reef_label(names(kml_data))\r\n",
        "  reef_ids_previous <- get_reef_label(names(previous_kml_data))\r\n",
        "  \r\n",
        "  # Add any reefs that did not exist in the previous kml but exist in the \r\n",
        "  # current kml\r\n",
        "  indices_not_in_previous <- which(!reef_ids %in% reef_ids_previous)\r\n",
        "  if(length(indices_not_in_previous) > 0){\r\n",
        "    for (i in 1:length(indices_not_in_previous)) {\r\n",
        "      name <- names(kml_data)[[i]]\r\n",
        "      spatial_differences[[name]] <- kml_data[[i]]\r\n",
        "    }\r\n",
        "  }\r\n",
        "\r\n",
        "  # Iterate through all IDS that exist in the new KML file. Any that have \r\n",
        "  # changed or do not exist in the previous data set will be added to the new \r\n",
        "  # one. It is intentional that cull sites that have been removed are not \r\n",
        "  # included, as a result of the general philosophy outlined in the documentation\r\n",
        "  for (reef_id in reef_ids) {\r\n",
        "    index_reefs <- which(reef_ids == reef_id)\r\n",
        "    index_reefs_previous <- which(reef_ids_previous == reef_id)\r\n",
        "    \r\n",
        "    if (length(index_reefs) > 0 && length(index_reefs_previous) > 0) {\r\n",
        "      reef <- kml_data[[index_reefs]]\r\n",
        "      reef_name <- names(kml_data)[[index_reefs]]\r\n",
        "      reef_previous <- previous_kml_data[[index_reefs_previous]]\r\n",
        "      reef_name_previous <- names(previous_kml_data)[[index_reefs_previous]]\r\n",
        "      \r\n",
        "      # If comparison fails or does not indicate that reefs are identical then \r\n",
        "      # add reef to list of changed reefs.\r\n",
        "      is_unchanged <- FALSE\r\n",
        "      tryCatch({\r\n",
        "        if (all(dim(reef) == dim(reef_previous))) {\r\n",
        "          is_unchanged <- (all(reef_previous == reef)) && (reef_name_previous == reef_name)\r\n",
        "        }\r\n",
        "\r\n",
        "      }, error = function(e) {\r\n",
        "        print(paste(\"Error: \",  conditionMessage(e)))\r\n",
        "      },\r\n",
        "      warning = function(w) {\r\n",
        "        print(paste(\"Warning: \",  conditionMessage(w)))\r\n",
        "      })\r\n",
        "      \r\n",
        "      if(!is_unchanged){\r\n",
        "        name <- names(kml_data)[[index_reefs]]\r\n",
        "        spatial_differences[[name]] <- kml_data[[index_reefs]]\r\n",
        "      }\r\n",
        "    }\r\n",
        "  }\r\n",
        "  return(spatial_differences)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "compute_checksum <- function(data) {\r\n",
        "  digest(data, algo = \"md5\", serialize = TRUE)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "assign_nearest_site_method_c <- function(data_df, kml_path, keyword, kml_path_previous=NULL, serialised_raster_path=NULL, spatial_output_path=NULL, raster_size=0.0005, x_closest=1, is_standardised=0, save_spatial_as_raster=0){\r\n",
        "  # Assign nearest sites to manta tows with method developed by Cameron Fletcher\r\n",
        "  \r\n",
        "  # Acquire the directory to store raster outputs and the most recent spatial \r\n",
        "  # file that was saved as an R binary\r\n",
        "  # if loading data fails calculate site regions\r\n",
        "  load_site_rasters_failed <- TRUE\r\n",
        "  if(!is.null(serialised_raster_path)){\r\n",
        "    if(file.info(serialised_raster_path)$isdir){\r\n",
        "      print(\"Invalid path to serialized spatial data. Must be a file not a directory\")\r\n",
        "      spatial_file <- NULL\r\n",
        "      spatial_directory <- serialised_raster_path\r\n",
        "    } else {\r\n",
        "      spatial_file <- serialised_raster_path\r\n",
        "    }\r\n",
        "\r\n",
        "    tryCatch({\r\n",
        "      base::message(\"Loading previously saved raster data ...\")\r\n",
        "      site_regions <- readRDS(spatial_file)\r\n",
        "      crs <- projection(site_regions[[1]])\r\n",
        "      load_site_rasters_failed <- FALSE\r\n",
        "      base::message(\"Loaded data successfully\")\r\n",
        "    }, error = function(e) {\r\n",
        "      print(paste(\"Error site regions could not be loaded. Site regions will be calculated instead.\", conditionMessage(e)))\r\n",
        "    })\r\n",
        "  }\r\n",
        "  \r\n",
        "  base::message(\"Loading kml file...\")\r\n",
        "  kml_layers <- st_layers(kml_path)\r\n",
        "  layer_names_vec <- unlist(kml_layers[\"name\"])\r\n",
        "  kml_data <- NULL\r\n",
        "  tryCatch({\r\n",
        "    plan(multisession)\r\n",
        "    future_layers <- future_map(layer_names_vec, ~ st_read(kml_path, layer = .x))\r\n",
        "    kml_data <- setNames(future_layers, layer_names_vec)\r\n",
        "  }, error = function(e) {\r\n",
        "    print(paste(\"Error KML not loaded with parrallel processing\", conditionMessage(e)))\r\n",
        "    kml_data <<- setNames(lapply(layer_names_vec, function(i)  st_read(kml_path, layer = i)), layer_names_vec)\r\n",
        "    return(kml_data)\r\n",
        "  })\r\n",
        "  kml_data <<- kml_data\r\n",
        "  base::message(\"Loaded KML file successfully\")\r\n",
        "  base::message(\"Compute checksum for kml\")\r\n",
        "  crs <- projection(kml_data[[1]])\r\n",
        "  sf_use_s2(FALSE)\r\n",
        "  checksum <- compute_checksum(kml_data)\r\n",
        "  \r\n",
        "  \r\n",
        "  # compare two kml files and return the geometry collections that have been \r\n",
        "  # updated \r\n",
        "  calculate_site_rasters <- TRUE\r\n",
        "  update_kml <- FALSE\r\n",
        "  if(!is.null(kml_path_previous) && !load_site_rasters_failed){\r\n",
        "    previous_kml_layers <- st_layers(kml_path_previous)\r\n",
        "    previous_layer_names_vec <- unlist(previous_kml_layers[\"name\"])\r\n",
        "    previous_kml_data <- setNames(lapply(previous_layer_names_vec, function(i)  st_read(kml_path_previous, layer = i)), previous_layer_names_vec)\r\n",
        "    previous_kml_data <<- previous_kml_data\r\n",
        "    previous_crs <- projection(previous_kml_data[[1]])\r\n",
        "    previous_checksum <- compute_checksum(previous_kml_data)\r\n",
        "    if(checksum != previous_checksum){\r\n",
        "      if(previous_crs == crs){\r\n",
        "        kml_data_to_update <- get_spatial_differences(kml_data, previous_kml_data)\r\n",
        "        tryCatch({\r\n",
        "          if(length(kml_data_to_update) == 0){\r\n",
        "            calculate_site_rasters <- FALSE\r\n",
        "          } else {\r\n",
        "            update_kml <- TRUE\r\n",
        "          }\r\n",
        "        }, error = function(e) {\r\n",
        "          update_kml <<- FALSE\r\n",
        "        })\r\n",
        "      } \r\n",
        "    } else {\r\n",
        "      base::message(\"Checksum determined current and previous KML data are identical\")\r\n",
        "      calculate_site_rasters <- FALSE\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  if(calculate_site_rasters){\r\n",
        "    if(!update_kml | load_site_rasters_failed){\r\n",
        "      \r\n",
        "      \r\n",
        "      tryCatch({\r\n",
        "        base::message(\"Simplifying kml polygons...\")\r\n",
        "        kml_data_simplified <- simplify_geometry_list_rdp(kml_data)\r\n",
        "        base::message(\"Simplified kml polygons successfully\")\r\n",
        "        \r\n",
        "      }, error = function(e) {\r\n",
        "        print(paste(\"Error Simplifying kml polygons\", conditionMessage(e)))\r\n",
        "        kml_data_simplified <<- kml_data\r\n",
        "      })\r\n",
        "      site_regions <- assign_raster_pixel_to_sites_original(kml_data_simplified, layer_names_vec, crs, raster_size, x_closest, is_standardised)\r\n",
        "    \r\n",
        "    } else {\r\n",
        "      base::message(\"Updating raster pixels for reefs that have changed since last process date...\")\r\n",
        "      \r\n",
        "      tryCatch({\r\n",
        "        base::message(\"Simplifying kml polygons...\")\r\n",
        "        kml_data_simplified <- simplify_geometry_list_rdp(kml_data_to_update)\r\n",
        "        base::message(\"Simplified kml polygons successfully\")\r\n",
        "      }, error = function(e) {\r\n",
        "        print(paste(\"Error Simplifying kml polygons\", conditionMessage(e)))\r\n",
        "        kml_data_simplified <<- kml_data_to_update\r\n",
        "      })\r\n",
        "      \r\n",
        "      tryCatch({\r\n",
        "        base::message(\"Number of Rasters to Update: \", length(kml_data_simplified))\r\n",
        "        updated_layer_names_vec <- names(kml_data_simplified)\r\n",
        "        updated_site_regions <- assign_raster_pixel_to_sites_original(kml_data_simplified, updated_layer_names_vec, crs, raster_size, x_closest, is_standardised)\r\n",
        "        \r\n",
        "        site_regions_reef_ids <- get_reef_label(names(kml_data))\r\n",
        "        updated_site_regions_reef_ids <- get_reef_label(names(kml_data_to_update))\r\n",
        "        \r\n",
        "        index_reefs <- match(updated_site_regions_reef_ids, site_regions_reef_ids)\r\n",
        "        site_regions <- site_regions[-index_reefs]\r\n",
        "        site_regions <- c(site_regions, updated_site_regions)\r\n",
        "        site_regions <- site_regions[order(names(site_regions))]\r\n",
        "      }, error = function(e) {\r\n",
        "        print(paste(\"Error updating updating raster pixels for reefs. Will create new raster\", conditionMessage(e)))\r\n",
        "        \r\n",
        "        tryCatch({\r\n",
        "          base::message(\"Simplifying kml polygons...\")\r\n",
        "          kml_data_simplified <- simplify_geometry_list_rdp(kml_data)\r\n",
        "          base::message(\"Simplified kml polygons successfully\")\r\n",
        "        }, error = function(e) {\r\n",
        "          print(paste(\"Error Simplifying kml polygons\", conditionMessage(e)))\r\n",
        "          kml_data_simplified <<- kml_data\r\n",
        "        })\r\n",
        "        site_regions <<- assign_raster_pixel_to_sites_original(kml_data_simplified, layer_names_vec, crs, raster_size, x_closest, is_standardised)\r\n",
        "      })\r\n",
        "\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  base::message(\"Saving raster data as serialised binary file...\")\r\n",
        "  tryCatch({\r\n",
        "    if (!dir.exists(spatial_output_path)) {\r\n",
        "      dir.create(spatial_output_path, recursive = TRUE)\r\n",
        "    }\r\n",
        "    saveRDS(site_regions, file.path(spatial_output_path, paste(\"site_regions_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".rds\", sep = \"\")))\r\n",
        "    contribute_to_metadata_report(\"output\", file.path(getwd(), paste(keyword,\"_site_regions_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".rds\", sep = \"\")))\r\n",
        "  }, error = function(e) {\r\n",
        "    print(paste(\"Error saving site regions as rds - Data saved in source directory\", conditionMessage(e)))\r\n",
        "    saveRDS(site_regions, paste(keyword,\"_site_regions_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".rds\", sep = \"\"))\r\n",
        "    contribute_to_metadata_report(\"output\", file.path(getwd(), paste(keyword,\"_site_regions_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".rds\", sep = \"\")))\r\n",
        "  })\r\n",
        "  \r\n",
        "  tryCatch({\r\n",
        "    if(save_spatial_as_raster == 1 & !is.null(spatial_output_path)){\r\n",
        "      base::message(\"Saving raster data as gtiff files...\")\r\n",
        "      spatial_file <- file.path(spatial_output_path, paste(\"site_regions_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".rds\", sep = \"\"))\r\n",
        "      raster_output <- file.path(spatial_output_path, \"rasters\")\r\n",
        "      if (!dir.exists(raster_output)) {\r\n",
        "        dir.create(raster_output, recursive = TRUE)\r\n",
        "      }\r\n",
        "      save_spatial_as_raster(raster_output, spatial_file)\r\n",
        "    }\r\n",
        "  }, error = function(e) {\r\n",
        "    print(paste(\"Error saving as rasters\", conditionMessage(e)))\r\n",
        "  })\r\n",
        "  \r\n",
        "  \r\n",
        "  base::message(\"Assigning sites to data...\")\r\n",
        "  data_df <- get_centroids(data_df, crs)\r\n",
        "  updated_pts <- data_df\r\n",
        "  for(i in 1:length(site_regions)){\r\n",
        "    is_contained <- sapply(data_df$`Reef ID`, function(str) grepl(str, names(site_regions[i])))\r\n",
        "    if(any(is_contained) == FALSE){\r\n",
        "      next\r\n",
        "    }\r\n",
        "    reef_pts <- data_df[is_contained,]\r\n",
        "    nearest_site_manta_data <- raster::extract(site_regions[[i]], reef_pts)\r\n",
        "    updated_pts[is_contained, c(\"Nearest Site\")] <- nearest_site_manta_data\r\n",
        "  }\r\n",
        "  updated_pts <- st_drop_geometry(updated_pts)\r\n",
        "  is_nearest_site_has_error <- is.na(updated_pts$`Nearest Site`) | ifelse(is.na(updated_pts$`Nearest Site` == -1),FALSE,updated_pts$`Nearest Site` == -1)\r\n",
        "  updated_pts[,\"error_flag\"] <- as.integer(updated_pts[,\"error_flag\"] | is_nearest_site_has_error)\r\n",
        "  \r\n",
        "  return(updated_pts)\r\n",
        "}\r\n",
        "\r\n",
        "get_centroids <- function(data_df, crs, precision=0){\r\n",
        "  # Determine the centroid coordinates create geospatial points or create \r\n",
        "  # geospatial points if only a single set of coordinates exist for an \r\n",
        "  # observation\r\n",
        "  \r\n",
        "  if (all(c(\"Start Lat\", \"End Lat\", \"End Lng\", \"Start Lng\") %in% colnames(data_df))){\r\n",
        "    data_df <- data_df %>%\r\n",
        "      mutate(\r\n",
        "        mean_lat = (`Start Lat` + `End Lat`) / 2,\r\n",
        "        mean_long = (`Start Lng` + `End Lng`) / 2\r\n",
        "      )\r\n",
        "    \r\n",
        "    if(precision != 0){\r\n",
        "      data_df <- data_df %>%\r\n",
        "        mutate_at(vars(`Start Lat`, `Start Lng`, `End Lat`, `End Lng`, `mean_lat`, `mean_long`), ~ round(., precision))\r\n",
        "    }\r\n",
        "  } else {\r\n",
        "    return(data_df)\r\n",
        "  }\r\n",
        "  \r\n",
        "  # Can't create pts that are NA, use 0 as place holder.\r\n",
        "  # the st_as_sf function by default removes these columns.\r\n",
        "  data_df$mean_lat[is.na(data_df$mean_lat)] <- 0\r\n",
        "  data_df$mean_long[is.na(data_df$mean_long)] <- 0\r\n",
        "  pts <- st_as_sf(data_df, coords=c(\"mean_long\", \"mean_lat\"), crs=crs)\r\n",
        "  return(pts)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "assign_raster_pixel_to_sites_multithread <- function(kml_data, layer_names_vec, crs, raster_size, x_closest=1, is_standardised=0){\r\n",
        "  # Produces same result as assign_raster_pixel_to_sites in a parallel manner\r\n",
        "  \r\n",
        "  if(is_standardised){\r\n",
        "    expanded_extent <- standardise_extents(kml_data)\r\n",
        "  } else {\r\n",
        "    expanded_extent <- list()\r\n",
        "    for(i in 1:length(kml_data)) {\r\n",
        "      expanded_extent[[i]] <- extent(kml_data[[i]])\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  # Define the increase amount in both x and y directions. \r\n",
        "  increase_amount <- 0.003\r\n",
        "  expanded_bboxs <- setNames(lapply(expanded_extent, function(i) {\r\n",
        "    \r\n",
        "    original_extent <- i\r\n",
        "    # Increase the extent by the specified amount\r\n",
        "    expanded_bboxs <- extent(original_extent[1] - increase_amount,\r\n",
        "                             original_extent[2] + increase_amount,\r\n",
        "                             original_extent[3] - increase_amount,\r\n",
        "                             original_extent[4] + increase_amount)\r\n",
        "    \r\n",
        "  }), layer_names_vec)\r\n",
        "  \r\n",
        "  # Create an empty raster based on the bounding box, cell size, and projection\r\n",
        "  rasters <- create_raster_templates(expanded_bboxs, layer_names_vec, crs, raster_size)\r\n",
        "  \r\n",
        "  # The original script in mathmatica utilised a function \"RegionDistance\". This \r\n",
        "  # finds the Euclidean distance between the polygon and a point and does not\r\n",
        "  # consider the curviture of the earth. \r\n",
        "  \r\n",
        "  site_regions <- foreach(i = 1:length(kml_data), .combine = \"c\",  .export = c(\"assign_raster_pixel_to_sites_single\", \"rasterToPoints\", \"st_polygon\", \"raster\", \"values\", \"as.matrix\", \"st_multipoint\", \"st_sfc\", \"st_cast\", \"st_crs<-\", \"st_distance\", \"drop_units\", \"site_names_to_numbers\", \"xth_smallest\")) %dopar% {\r\n",
        "    assign_raster_pixel_to_sites_single_reef(rasters[[i]], kml_data[[i]], crs, x_closest)\r\n",
        "  }\r\n",
        "  site_regions <- setNames(site_regions, layer_names_vec)\r\n",
        "  return(site_regions)\r\n",
        "}\r\n",
        "\r\n",
        "assign_raster_pixel_to_sites_single_reef <- function(raster, site_poly, crs, x_closest){\r\n",
        "  \r\n",
        "  raster::values(raster) <- 1\r\n",
        "  pixel_coords <- rasterToPoints(raster)\r\n",
        "  pixel_coords <- pixel_coords[,1:2]\r\n",
        "  raster_points <- pixel_coords |> as.matrix() |> st_multipoint() |> st_sfc() |> st_cast('POINT')\r\n",
        "  st_crs(raster_points) <- crs\r\n",
        "  \r\n",
        "  site_names <- site_poly$Name\r\n",
        "  site_numbers <- site_names_to_numbers(site_names)\r\n",
        "  \r\n",
        "  distances <- st_distance(site_poly, raster_points)\r\n",
        "  \r\n",
        "  if (dim(distances)[1] != 1){\r\n",
        "    distances <- apply(distances, 2, as.numeric)\r\n",
        "    min_distances_list <- apply(distances, 2, xth_smallest, x_values=x_closest)\r\n",
        "    min_distances_df <- do.call(rbind, min_distances_list)\r\n",
        "    min_distance_site_numbers <- site_numbers[as.vector(min_distances_df$`Nearest Site`)]\r\n",
        "    min_distances <- min_distances_df$`Distance to Site`\r\n",
        "  } else {\r\n",
        "    min_distances <- drop_units(distances)\r\n",
        "    min_distance_site_numbers <- rep(site_numbers, length(min_distances))\r\n",
        "  }\r\n",
        "  \r\n",
        "  is_within_required_distance <- min_distances > 300\r\n",
        "  min_distance_site_numbers[is_within_required_distance] <- -1\r\n",
        "  raster::values(raster) <- min_distance_site_numbers\r\n",
        "  names(raster) <- c(\"Nearest Site\")\r\n",
        "  \r\n",
        "  return(raster)\r\n",
        "}\r\n",
        "\r\n",
        "assign_raster_pixel_to_sites <- function(kml_data, layer_names_vec, crs, raster_size, x_closest=1, is_standardised=0){\r\n",
        "  site_regions <- NULL\r\n",
        "  tryCatch({\r\n",
        "    library(foreach)\r\n",
        "    library(doParallel)\r\n",
        "    \r\n",
        "    cl <- makeCluster(detectCores())\r\n",
        "    registerDoParallel(cl)\r\n",
        "    \r\n",
        "    site_regions <- assign_raster_pixel_to_sites_multithread(kml_data, layer_names_vec, crs, raster_size, x_closest=1, is_standardised=0)\r\n",
        "    \r\n",
        "  }, error = function(e) {\r\n",
        "    base::message(\"Failed to perform assigning of sites with multithreading. Performing without...\")\r\n",
        "    site_regions <<- assign_raster_pixel_to_sites_original(kml_data, layer_names_vec, crs, raster_size, x_closest=1, is_standardised=0)\r\n",
        "  })\r\n",
        "  return(site_regions)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "assign_raster_pixel_to_sites_original <- function(kml_data, layer_names_vec, crs, raster_size, x_closest=1, is_standardised=0){\r\n",
        "  # This is a method of assigning sites to manta tows that was initially \r\n",
        "  # implemented in Mathmatica by Dr Cameron Fletcher. A set of rasters are \r\n",
        "  # created slightly larger than the bounding box of each layer in the KML file.\r\n",
        "  # The rasters are convereted into a matrix of geospatial \"POINTS\" to compare \r\n",
        "  # the distance between each point and the sites on the corresponding layer. \r\n",
        "  # Each raster pixel will then be assigned a value corresponding with the \r\n",
        "  # nearest site (geodesic distance) \r\n",
        "  \r\n",
        "  if(is_standardised){\r\n",
        "    expanded_extent <- standardise_extents(kml_data)\r\n",
        "  } else {\r\n",
        "    expanded_extent <- list()\r\n",
        "    for(i in 1:length(kml_data)) {\r\n",
        "      expanded_extent[[i]] <- extent(kml_data[[i]])\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  # Define the increase amount in both x and y directions. \r\n",
        "  increase_amount <- 0.003\r\n",
        "  expanded_bboxs <- setNames(lapply(expanded_extent, function(i) {\r\n",
        "    \r\n",
        "    original_extent <- i\r\n",
        "    # Increase the extent by the specified amount\r\n",
        "    expanded_bboxs <- extent(original_extent[1] - increase_amount,\r\n",
        "                             original_extent[2] + increase_amount,\r\n",
        "                             original_extent[3] - increase_amount,\r\n",
        "                             original_extent[4] + increase_amount)\r\n",
        "    \r\n",
        "  }), layer_names_vec)\r\n",
        "  \r\n",
        "  # Create an empty raster based on the bounding box, cell size, and projection\r\n",
        "  rasters <- create_raster_templates(expanded_bboxs, layer_names_vec, crs, raster_size)\r\n",
        "  \r\n",
        "  # The original script in mathmatica utilised a function \"RegionDistance\". This \r\n",
        "  # finds the Euclidean distance between the polygon and a point and does not\r\n",
        "  # consider the curviture of the earth. \r\n",
        "  site_regions <- setNames(vector(\"list\", length=length(rasters)),layer_names_vec)\r\n",
        "  \r\n",
        "  for (i in seq_along(rasters)) {\r\n",
        "    raster <- rasters[[i]]\r\n",
        "    site_poly <- kml_data[[i]]\r\n",
        "    \r\n",
        "    # set all values of the raster layer so every pixel can be exported to a \r\n",
        "    # dataframe and then converted to points \r\n",
        "    values(raster) <- 1\r\n",
        "    pixel_coords <- rasterToPoints(raster)\r\n",
        "    pixel_coords <- pixel_coords[,1:2]\r\n",
        "    raster_points <- pixel_coords |> as.matrix() |> st_multipoint() |> st_sfc() |> st_cast('POINT')\r\n",
        "    st_crs(raster_points) <- crs\r\n",
        "    \r\n",
        "    # Get site numbers\r\n",
        "    site_names <- site_poly$Name\r\n",
        "    site_numbers <- site_names_to_numbers(site_names)\r\n",
        "    \r\n",
        "    # Can determine the Euclidean and geodesic distance (st_distance)\r\n",
        "    distances <- st_distance(site_poly, raster_points)\r\n",
        "    if (dim(distances)[1] != 1){\r\n",
        "      \r\n",
        "      distances <- apply(distances, 2, as.numeric)\r\n",
        "      min_distances_list <- apply(distances, 2, xth_smallest, x_values=x_closest)\r\n",
        "      min_distances_df <- do.call(rbind, min_distances_list)\r\n",
        "      min_distance_site_numbers <- site_numbers[as.vector(min_distances_df$`Nearest Site`)]\r\n",
        "      min_distances <- min_distances_df$`Distance to Site`\r\n",
        "      \r\n",
        "    } else {\r\n",
        "      min_distances <- drop_units(distances)\r\n",
        "      min_distance_site_numbers <- rep(site_numbers, length(min_distances))\r\n",
        "    }\r\n",
        "    \r\n",
        "    # Set site numbers to NA if they are more than 300m away.\r\n",
        "    is_within_required_distance <- min_distances > 300\r\n",
        "    min_distance_site_numbers[is_within_required_distance] <- -1\r\n",
        "    values(raster) <- min_distance_site_numbers\r\n",
        "    names(raster) <- c(\"Nearest Site\")\r\n",
        "    site_regions[[i]] <- raster\r\n",
        "  }\r\n",
        "  \r\n",
        "  return(site_regions)\r\n",
        "}\r\n",
        "\r\n",
        "site_names_to_numbers <- function(site_names){\r\n",
        "  return(as.numeric(sub(\".+_(\\\\d+).*\", \"\\\\1\", site_names)))\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "simplify_shp_polyogns_rdp <- function(shapefile){\r\n",
        "  # simplify polygons stored in a shapefile with the Ramer-Douglas-Peucker \r\n",
        "  # algorithm\r\n",
        "  \r\n",
        "  reef_geometries <- shapefile_filtered[,\"geometry\"]\r\n",
        "  simplified_shapefile_filtered <- shapefile_filtered\r\n",
        "  reef_geometries_updated <- reef_geometries\r\n",
        "  for(i in 1:nrow(reef_geometries)){\r\n",
        "    # A vast majority of reef_geometries at this level are polygons but \r\n",
        "    # occasionally they are geometrycollections and require iteration. \r\n",
        "    \r\n",
        "    site_polygon <- reef_geometries[i,1][[1]][[1]]\r\n",
        "    polygon_points <- site_polygon[[1]]\r\n",
        "    approx_polygon_points <- polygon_rdp(polygon_points) \r\n",
        "    site_polygon[[1]] <- approx_polygon_points\r\n",
        "    reef_geometries[i,1][[1]][[1]] <- site_polygon\r\n",
        "    \r\n",
        "  }\r\n",
        "  simplified_shapefile_filtered <- st_drop_geometry(simplified_shapefile_filtered)\r\n",
        "  simplified_shapefile_filtered <- st_as_sf(simplified_shapefile_filtered, geometry = reef_geometries$geometry)\r\n",
        "  \r\n",
        "  return(simplified_shapefile_filtered)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "simplify_geometry_list_rdp <- function(kml_data){\r\n",
        "  # simplify all polygons in a list that was retrieved from the kml file \r\n",
        "  # the Ramer-Douglas-Peucker algorithm\r\n",
        "  \r\n",
        "  simplified_kml_data <- kml_data\r\n",
        "  for (j in 1:length(kml_data)){\r\n",
        "    reef_geometries <- kml_data[[j]][[\"geometry\"]]\r\n",
        "    simplified_kml_data[[j]][[\"geometry\"]] <- simplify_geometry_rec(reef_geometries)\r\n",
        "    \r\n",
        "  }\r\n",
        "  return(simplified_kml_data)\r\n",
        "}\r\n",
        "\r\n",
        "# This is a recursive function that takes a geometry and recursively finds and \r\n",
        "# simplifies polygons with the Ramer-Douglas-Peucker algorithm.\r\n",
        "simplify_geometry_rec <- function(geometry){\r\n",
        "  for (i in 1:length(geometry)){\r\n",
        "    polygon <- geometry[[i]]\r\n",
        "    geometry_type <- NULL\r\n",
        "    tryCatch({\r\n",
        "      geometry_type <- st_geometry_type(polygon)\r\n",
        "    }, error = function(e) {\r\n",
        "      geometry_type <<- \"LIST\"\r\n",
        "    })\r\n",
        "    \r\n",
        "    if(geometry_type %in% c(\"GEOMETRYCOLLECTION\", \"MULTIPOLYGON\")){\r\n",
        "      polygon <- simplify_geometry_rec(polygon)\r\n",
        "    } else if (!(geometry_type %in% c(\"POINT\"))){\r\n",
        "      polygon_points <- polygon[[1]]\r\n",
        "      approx_polygon_points <- polygon_rdp(polygon_points)\r\n",
        "      polygon[[1]] <- approx_polygon_points\r\n",
        "    }\r\n",
        "    geometry[[i]] <- polygon\r\n",
        "  }\r\n",
        "  return(geometry)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "polygon_rdp <- function(polygon_points, epsilon=0.00001) {\r\n",
        "  # adaptation of the Ramer-Douglas-Peucker algorithm. The original algorithm \r\n",
        "  # was developed for a use with a line not a ploygon. Remove the last point \r\n",
        "  # temporarily, perform the algorithm and then return the value.\r\n",
        "  \r\n",
        "  line_points <- polygon_points[1:nrow(polygon_points)-1,]\r\n",
        "  approx_line_points <- rdp(line_points, epsilon)\r\n",
        "  polygon <- rbind(approx_line_points, approx_line_points[1,])\r\n",
        "  return(polygon)\r\n",
        "}\r\n",
        "\r\n",
        "rdp <- function(points, epsilon=0.00001) {\r\n",
        "  # Ramer-Douglas-Peucker algorithm s\r\n",
        "  if (nrow(points) <= 2) {\r\n",
        "    return(points)\r\n",
        "  }\r\n",
        "  \r\n",
        "  dmax <- 0\r\n",
        "  index <- 0\r\n",
        "  end <- nrow(points)\r\n",
        "  \r\n",
        "  # Find the point with the maximum distance\r\n",
        "  for (i in 2:(end - 1)) {\r\n",
        "    d <- perpendicularDistance(points[i,], points[1,], points[end,])\r\n",
        "    if (d > dmax) {\r\n",
        "      index <- i\r\n",
        "      dmax <- d\r\n",
        "    }\r\n",
        "  }\r\n",
        "  \r\n",
        "  \r\n",
        "  result <- matrix(nrow = 0, ncol = ncol(points))\r\n",
        "  # If max distance is greater than epsilon, recursively simplify\r\n",
        "  if (dmax > epsilon) {\r\n",
        "    recursive1 <- rdp(points[1:index,], epsilon)\r\n",
        "    recursive2 <- rdp(points[(index):end,], epsilon)\r\n",
        "    result <- rbind(result, rbind(recursive1[1:nrow(recursive1) - 1,], recursive2))\r\n",
        "  } else {\r\n",
        "    result <- rbind(points[1,], points[end,])\r\n",
        "  }\r\n",
        "  \r\n",
        "  return(result)\r\n",
        "}\r\n",
        "\r\n",
        "# Calculate perpendicular distance of a point p from a line segment AB\r\n",
        "perpendicularDistance <- function(p, A, B) {\r\n",
        "  numerator <- abs((B[2] - A[2]) * p[1] - (B[1] - A[1]) * p[2] + B[1] * A[2] - B[2] * A[1])\r\n",
        "  denominator <- sqrt((B[2] - A[2])^2 + (B[1] - A[1])^2)\r\n",
        "  result <- (numerator / denominator)\r\n",
        "  return(result)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "find_largest_extent <- function(kml_data){\r\n",
        "  num_geometries <- length(kml_data)\r\n",
        "  \r\n",
        "  # Preallocate the data frame\r\n",
        "  result <- data.frame(\r\n",
        "    x_difference = numeric(num_geometries),\r\n",
        "    y_difference = numeric(num_geometries)\r\n",
        "  )\r\n",
        "  \r\n",
        "  for (i in 1:num_geometries) {\r\n",
        "    bbox <- extent(kml_data[[i]])\r\n",
        "    diff_x <- bbox@xmax - bbox@xmin\r\n",
        "    diff_y <- bbox@ymax - bbox@ymin\r\n",
        "    result[i, \"x_difference\"] <- diff_x\r\n",
        "    result[i, \"y_difference\"] <- diff_y\r\n",
        "  }\r\n",
        "  \r\n",
        "  colnames(result) <- c(\"x_difference\", \"y_difference\")\r\n",
        "  largest_extent <- c(max(result$x_difference),max(result$y_difference))\r\n",
        "  names(largest_extent) <- c(\"x_difference\", \"y_difference\")\r\n",
        "  \r\n",
        "  return(largest_extent)\r\n",
        "}\r\n",
        "\r\n",
        "standardise_extents <- function(kml_data){\r\n",
        "  largest_extent <- find_largest_extent(kml_data)\r\n",
        "  adjusted_extents <- list()\r\n",
        "  for(i in 1:length(kml_data)) {\r\n",
        "    extent <- extent(kml_data[[i]])\r\n",
        "    diff_x <- extent@xmax - extent@xmin\r\n",
        "    diff_y <- extent@ymax - extent@ymin\r\n",
        "    largest_extent_centered <- extent(extent@xmin + diff_x/2 - largest_extent[1]/2,\r\n",
        "                                      extent@xmax - diff_x/2 + largest_extent[1]/2,\r\n",
        "                                      extent@ymin + diff_y/2 - largest_extent[2]/2,\r\n",
        "                                      extent@ymax - diff_y/2 + largest_extent[2]/2\r\n",
        "    )\r\n",
        "    adjusted_extents[[i]] <- largest_extent_centered\r\n",
        "  }\r\n",
        "  return(adjusted_extents)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "create_raster_templates <- function(extents, layer_names_vec, crs, raster_size=150){\r\n",
        "  \r\n",
        "  # rasterise the bounding boxes \r\n",
        "  # res <- 0.0005\r\n",
        "  # NN input requires standard image size. 570x550 is approximately a resolution of 0.00045\r\n",
        "  if (raster_size > 1){\r\n",
        "    y_pixel <- raster_size\r\n",
        "    x_pixel <- raster_size\r\n",
        "    rasters <- setNames(lapply(extents, function(i) {\r\n",
        "      raster(ext = i, ncols=x_pixel, nrows=y_pixel, crs = crs)\r\n",
        "    }), layer_names_vec)\r\n",
        "  } else {\r\n",
        "    rasters <- setNames(lapply(extents, function(i) {\r\n",
        "      raster(ext = i, resolution=raster_size, crs = crs)\r\n",
        "    }), layer_names_vec)\r\n",
        "  }\r\n",
        "  return(rasters)\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "rasterise_sites <- function(kml_data, is_standardised=1, raster_size=150){\r\n",
        "  if (is_standardised == 1){\r\n",
        "    extent_data <- standardise_extents(kml_data)\r\n",
        "    location <- \"standardised_extent\"\r\n",
        "  } else {\r\n",
        "    extent_data <- kml_data\r\n",
        "    location <- \"varying_extent\"\r\n",
        "  }\r\n",
        "  \r\n",
        "  # res <- 0.0005\r\n",
        "  # NN input requires standard image size. 570x550 is approximately a resolution of 0.00045\r\n",
        "  y_pixel <- raster_size\r\n",
        "  x_pixel <- raster_size\r\n",
        "  for(i in 1:length(kml_data)){\r\n",
        "    site_names <- kml_data[[i]]$Name\r\n",
        "    site_numbers <- site_names_to_numbers(site_names)\r\n",
        "    kml_data[[i]]$site_number <- site_numbers\r\n",
        "    site_raster <- st_rasterize(kml_data[[i]], st_as_stars(st_bbox(extent_data[[i]]), field = \"site_number\", nx = x_pixel, ny = y_pixel))\r\n",
        "    file_name <- names(kml_data[i])\r\n",
        "    modified_file_name <- gsub(\"/\", \"_\", file_name)\r\n",
        "    site_raster <- as(site_raster, \"Raster\")\r\n",
        "    writeRaster(site_raster, filename = paste(\"CNN\\\\\", raster_size, \"\\\\\", location,  \"\\\\sites_as_rasters\\\\\", modified_file_name, sep=\"\"), format = \"GTiff\", overwrite = TRUE)\r\n",
        "    site_raster <- NA\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "rasterise_sites_reef_encoded <- function(kml_data, layer_names_vec, is_standardised=1, raster_size=150){\r\n",
        "  \r\n",
        "  if (is_standardised == 1){\r\n",
        "    extent_data <- standardise_extents(kml_data)\r\n",
        "    location <- \"standardised_extent\"\r\n",
        "  } else {\r\n",
        "    extent_data <- kml_data\r\n",
        "    location <- \"varying_extent\"\r\n",
        "  }\r\n",
        "  # res <- 0.0005\r\n",
        "  # NN input requires standard image size. 570x550 is approximately a resolution of 0.00045\r\n",
        "  y_pixel <- raster_size\r\n",
        "  x_pixel <- raster_size\r\n",
        "  for(i in 1:length(kml_data)){\r\n",
        "    site_names <- kml_data[[i]]$Name\r\n",
        "    site_numbers <- site_names_to_numbers(site_names)\r\n",
        "    reef_numbers <- as.numeric(gsub(\"[^0-9.]\", \"\", layer_names_vec[[i]]))\r\n",
        "    encoded_reef_site_numbers <- as.numeric(sapply(site_numbers, function (x) paste(reef_numbers, x, sep = \"\")))\r\n",
        "    kml_data[[i]]$site_number <- encoded_reef_site_numbers\r\n",
        "    site_raster <- st_rasterize(kml_data[[i]], st_as_stars(st_bbox(extent_data[[i]]), field = \"site_number\", nx = x_pixel, ny = y_pixel))\r\n",
        "    file_name <- names(kml_data[i])\r\n",
        "    modified_file_name <- gsub(\"/\", \"_\", file_name)\r\n",
        "    site_raster <- as(site_raster, \"Raster\")\r\n",
        "    writeRaster(site_raster, filename = paste(\"CNN\\\\\", raster_size, \"\\\\\", location, \"\\\\sites_as_rasters_encoded\\\\\", modified_file_name, sep=\"\"), format = \"GTiff\", overwrite = TRUE)\r\n",
        "    site_raster <- NA\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "xth_smallest <- function(x, x_values) {\r\n",
        "  # Function to find the xth smallest value in a vector without sorting. This \r\n",
        "  # allows for the second closest sites etc to be determined. Likely unnecessary \r\n",
        "  #in production was used for testing purposes\r\n",
        "  \r\n",
        "  sorted_uniques <- sort(x)\r\n",
        "  xth_smallest_values <- sorted_uniques[x_values]\r\n",
        "  xth_smallest_indices <- which(x %in% xth_smallest_values)\r\n",
        "  if(length(xth_smallest_indices > 1)){\r\n",
        "    xth_smallest_indices <- xth_smallest_indices[1]\r\n",
        "  }\r\n",
        "  output <- data.frame(matrix(ncol = (2*length(x_values))))\r\n",
        "  colnames(output) <- c(\"Nearest Site\", \"Distance to Site\")\r\n",
        "  output[1,] <- c(xth_smallest_indices, xth_smallest_values)\r\n",
        "  return(output)\r\n",
        "  \r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "send_error_email <- function(oauth_path, to_email, content, subject = \"Fatal Error in CCIP Control Data Workflow\") {\r\n",
        "  tryCatch({\r\n",
        "    if (file.info(oauth_path)$isdir) {\r\n",
        "      files <- list.files(oauth_path, full.names = TRUE)\r\n",
        "      if (length(files) > 0) {\r\n",
        "        file_info <- file.info(files)\r\n",
        "        oauth_path <- files[which.max(file_info$mtime)]\r\n",
        "      }\r\n",
        "    }\r\n",
        "    auth <- fromJSON(oauth_path)\r\n",
        "    gmail_auth(scope = \"compose\", id = auth$installled$client_id, secret = auth$installled$client_secret)\r\n",
        "    # Format email with the error message\r\n",
        "    error_message <- paste(\"Error in R Script:\\n\", conditionMessage(content))\r\n",
        "    mime <- create_mime(\r\n",
        "      To(to_email),\r\n",
        "      Subject(subject),\r\n",
        "      body = error_message\r\n",
        "    )\r\n",
        "    \r\n",
        "    # Send the email\r\n",
        "    send_message(mime)\r\n",
        "    cat(\"Error email sent.\\n\")\r\n",
        "  }, error = function(e) {\r\n",
        "    print(\"Error email failed.\\n\")\r\n",
        "  })\r\n",
        "}\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Main"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process GBRMPA control data into a standardised format that researchers have utilised\r\n",
        "# historically. Verification functions were written in consultation with domain experts \r\n",
        "# determine whether a row contains an error and is suitable for use in research. \r\n",
        "# The ecological observations are assigned to the nearest site.\r\n",
        "main <- function(new_path, configuration_path = NULL, kml_path = NULL, leg_path = NULL) {\r\n",
        "## `new_path` = Path to the file containing the new control data. This new file should \r\n",
        "# contain the entire historical dataset exported from GBRMPA EOTR database\r\n",
        "## `configuration_path` = Path to JSON configuration file utilised to specify desired \r\n",
        "# setting. This has a specific format and should remain consistent with historically \r\n",
        "# devloped config files. \r\n",
        "## `kml_path` = Path to kml spatial file containing the GBRMPA cull sites. Polygons \r\n",
        "# within this file are the basis for determining the nearest site. \r\n",
        "## `leg_path` = Path to legacy control data. This is a CSV contain data previously \r\n",
        "# processed by this workflow. It is used as a point of comparison and is optional. \r\n",
        "\r\n",
        "  \r\n",
        "  tryCatch({\r\n",
        "\r\n",
        "    ### Initialize Required Variables -------------------------------------------------------------\r\n",
        "    # Get the keyword from the new input file used to identify what type of control \r\n",
        "    # data it is. \r\n",
        "    keyword <- get_file_keyword(new_path) \r\n",
        "\r\n",
        "    # When a configuration file is not specified specifically with a path then the \r\n",
        "    # most recent config file with the keyword is utilsied \r\n",
        "    if (is.null(configuration_path)) {\r\n",
        "      configuration_path <- find_recent_file(\"configuration_files/\", paste(\"research_\",keyword, sep=\"\"), \"json\")\r\n",
        "      configuration <- fromJSON(configuration_path)\r\n",
        "    }\r\n",
        "\r\n",
        "    # If parameters are not provided the workflow checks the expected location for \r\n",
        "    # the most recent file that meets the requirements. \r\n",
        "    most_recent_kml_path <- find_recent_file(configuration$metadata$input_directory$spatial_data, \"Sites\", \"kml\")\r\n",
        "    most_recent_leg_path <- find_recent_file(configuration$metadata$output_directory$control_data_unaggregated, configuration$metadata$control_data_type, \"csv\")\r\n",
        "\r\n",
        "    # The workflow accesses the report and serialised raster data generated by the \r\n",
        "    # previous instance of this workflow. This information can be utilised later to\r\n",
        "    # reduce the number of spatial operations.\r\n",
        "    most_recent_report_path <- find_recent_file(configuration$metadata$output_directory$reports, configuration$metadata$control_data_type, \"json\")\r\n",
        "    serialised_spatial_path <- find_recent_file(configuration$metadata$output_directory$spatial_data, \"site_regions\", \"rds\")\r\n",
        "    separate_data <- configuration$metadata$separate_data\r\n",
        "\r\n",
        "    \r\n",
        "    previous_kml_path <- NULL\r\n",
        "    if(!is.null(most_recent_report_path)){\r\n",
        "      previous_report <- fromJSON(most_recent_report_path)\r\n",
        "      previous_kml_path <- previous_report$inputs$kml_path\r\n",
        "    } \r\n",
        "\r\n",
        "    # Attempt to use legacy data where possible. \r\n",
        "    # The workflow \"is_new\" if there is no evidence that the workflow has be run \r\n",
        "    # previously. \r\n",
        "    is_new <- 0\r\n",
        "    is_legacy_data_available <- 1\r\n",
        "    if (is.null(leg_path)) {\r\n",
        "      leg_path <- most_recent_leg_path\r\n",
        "      if(is.null(most_recent_leg_path)){\r\n",
        "        is_new <- 1\r\n",
        "        is_legacy_data_available <- 0\r\n",
        "      } else {\r\n",
        "        leg_path <- most_recent_leg_path\r\n",
        "      }\r\n",
        "    } \r\n",
        "    \r\n",
        "    if (is.null(kml_path)) {\r\n",
        "      kml_path <- most_recent_kml_path\r\n",
        "    }\r\n",
        "\r\n",
        "    ### Import Data -------------------------------------------------------------\r\n",
        "    # import new and legacy data. \r\n",
        "    # A legacy dataset with no error_flag column indicates it is new becuase R implementation \r\n",
        "    # of this workflow exports an error flag column, whereas the mathmatica implementation \r\n",
        "    # did not\r\n",
        "    new_data_df <- rio::import(new_path)\r\n",
        "    if(is_legacy_data_available){\r\n",
        "      legacy_df <- rio::import(leg_path)\r\n",
        "      if(\"error_flag\" %in% colnames(legacy_df)){\r\n",
        "        is_new <- 0\r\n",
        "      } else {\r\n",
        "        is_new <- 1\r\n",
        "        legacy_df[\"error_flag\"] <- 0\r\n",
        "      }\r\n",
        "    }\r\n",
        "    \r\n",
        "    # Check if the new data has an authoritative ID. All rows of a database export \r\n",
        "    # will have one and no rows from a powerBI export will. There should be no \r\n",
        "    # scenario where only a portion of rows have IDs. Even if an ID is present it \r\n",
        "    # should be ensured that it is authoritative before altering the configuration \r\n",
        "    # files to preference the use of the ID for separation rather than checking \r\n",
        "    # for differences manually. \r\n",
        "    \r\n",
        "    has_authorative_ID <-  !any(is.na(new_data_df[[configuration$metadata$ID_col]])) & configuration$metadata$is_ID_preferred\r\n",
        "    assign(\"has_authorative_ID\", has_authorative_ID, envir = .GlobalEnv) \r\n",
        "\r\n",
        "\r\n",
        "    ### Setup Report -------------------------------------------------------------\r\n",
        "    \r\n",
        "    # create list to export as json file with metadata about workflow\r\n",
        "    metadata_json_output <- list()    \r\n",
        "    \r\n",
        "    # timestamp workflow \r\n",
        "    metadata_json_output[[\"timestamp\"]] = Sys.time()\r\n",
        "    \r\n",
        "    # record inputs to workflow\r\n",
        "    metadata_json_output[[\"inputs\"]] = list(\r\n",
        "      kml_path = kml_path,\r\n",
        "      new_path = new_path,\r\n",
        "      leg_path = leg_path, \r\n",
        "      configuration_path = configuration_path,\r\n",
        "      serialised_spatial_path = serialised_spatial_path\r\n",
        "    )\r\n",
        "    \r\n",
        "    # record inputs to workflow\r\n",
        "    metadata_json_output[[\"decisions\"]] = list(\r\n",
        "      has_authorative_ID = has_authorative_ID,\r\n",
        "      is_legacy_data_available = is_legacy_data_available,\r\n",
        "      is_new = is_new\r\n",
        "    )\r\n",
        "    \r\n",
        "    # save metadata json file \r\n",
        "    json_data <- toJSON(metadata_json_output, pretty = TRUE)\r\n",
        "    writeLines(json_data, file.path(getwd(), configuration$metadata$output_directory$reports, paste(configuration$metadata$control_data_type, \"_Report_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".json\", sep = \"\")))\r\n",
        "    \r\n",
        "\r\n",
        "    ### Transform Data -------------------------------------------------------------\r\n",
        "    # Transform the new incoming data into the desired structure. Create new \r\n",
        "    # columns that are required but not present in the incoming dataset and \r\n",
        "    # populate them with defaul values that can be constant or derived from \r\n",
        "    # other columns in the dataset.\r\n",
        "    transformed_data_df <- map_data_structure(new_data_df, configuration$mappings$transformations, configuration$mappings$new_fields)\r\n",
        "\r\n",
        "    # Set the data type of both datasets to ensure that any operations or \r\n",
        "    # comparisons are executed in the expected manner. Rows that fail to parse \r\n",
        "    # indicate a fundamental flaw in the dataset and should be addressed by the\r\n",
        "    # providers of the dataset.\r\n",
        "    if(is_legacy_data_available){\r\n",
        "      legacy_df <- set_data_type(legacy_df, configuration$mappings$data_type_mappings)\r\n",
        "    }\r\n",
        "    formatted_data_df <- set_data_type(transformed_data_df, configuration$mappings$data_type_mappings) \r\n",
        "\r\n",
        "    # This function can help automate generating new config file versions\r\n",
        "    # adjusting to changes in the expected input. Currently has minor bugs \r\n",
        "    # and is optional but helpful\r\n",
        "    # configuration <- update_config_file(new_data_df, configuration_path)\r\n",
        "\r\n",
        "\r\n",
        "    ### Verify Data -------------------------------------------------------------\r\n",
        "    # verification functions flag whether a row is deemed to have an error. \r\n",
        "    # Although there may be situations where subsets of these rows can still \r\n",
        "    # be utilised it provides a quick way to filter out data not considered \r\n",
        "    # ideal for research purposes. This puts the onus on the researcher to \r\n",
        "    # justify their inclusion of error flagged data.\r\n",
        "    verified_data_df <- verify_entries(formatted_data_df, configuration)\r\n",
        "    if(is_new && is_legacy_data_available){\r\n",
        "      legacy_df <- verify_entries(legacy_df, configuration) \r\n",
        "    }\r\n",
        "    \r\n",
        "    # flag non-genuine duplicates that are mistakes. There are possible \r\n",
        "    # instances where two perfect matches are not duplicates but rather two \r\n",
        "    # identical valid data points (predominantly seen in cull data). \r\n",
        "    # Typically errorous data will be 3 or more duplications and as a result, \r\n",
        "    # this is the only situation where duplicates are flagged as errors. \r\n",
        "    verified_data_df <- flag_duplicates(verified_data_df)\r\n",
        "\r\n",
        "    ### Assign Sites to Data -------------------------------------------------------------\r\n",
        "    # This process determines the site number where manta tow and culls were performed.\r\n",
        "    # Although cull sites have the site name included in the dataset, including the \r\n",
        "    # site number makes it easier for researchers to compare the datasets.\r\n",
        "    tryCatch({\r\n",
        "      if(configuration$metadata$assign_sites){\r\n",
        "        if(configuration$metadata$control_data_type == \"manta_tow\"){\r\n",
        "\r\n",
        "          # Assigns sites to manta tows with method similar to intial mathmatica \r\n",
        "          # implementation.\r\n",
        "          verified_data_df <- assign_nearest_site_method_c(verified_data_df, kml_path, configuration$metadata$control_data_type, previous_kml_path, serialised_spatial_path, configuration$metadata$output_directory$spatial_data, raster_size=0.0005, x_closest=1, is_standardised=0, save_spatial_as_raster=0)\r\n",
        "        } else {\r\n",
        "          verified_data_df$`Nearest Site` <- site_names_to_numbers(verified_data_df$`Site Name`)\r\n",
        "        }\r\n",
        "        verified_data_df$`Nearest Site` <- ifelse(is.na(verified_data_df$`Nearest Site`), -1, verified_data_df$`Nearest Site`)\r\n",
        "      }\r\n",
        "    }, error = function(e) {\r\n",
        "      print(paste(\"Error assigning sites:\", conditionMessage(e)))\r\n",
        "    })\r\n",
        "\r\n",
        "\r\n",
        "    ### Separate Data -------------------------------------------------------------\r\n",
        "    # Datasets are provided as a single large tabular file which contain all control \r\n",
        "    # data from the inception of the COTS control program to present date. Consequently, \r\n",
        "    # a large portion of the rows will have been processed previously and will be seen \r\n",
        "    # in the legacy dataset. This provides the oppirtunity to seperate the new dataset \r\n",
        "    # into sections to perform further error checking. \r\n",
        "\r\n",
        "    # The three sections that the data from \"new_path\" can be split into are, new, perfect\r\n",
        "    # match and discrepancy. New is data that this workflow has never seen before. perfect \r\n",
        "    # match is data that is present in the legacy data set and has therefore been processed \r\n",
        "    # previously by this workflow. Discrepancy is a row deemed to have been processed\r\n",
        "    # previously but has has minor changes since which could be either quality assurance or \r\n",
        "    # a mistake. \r\n",
        "\r\n",
        "    # Separating the data can provide the oppirtunity to identify and prevent discrepancies  \r\n",
        "    # that transition from having no flagged error in the legacy dataset to a flagged error \r\n",
        "    # as this is likely a mistake. This process can be time consuming and is up to the \r\n",
        "    # user to determine if the trade off is beneficial for their applications. \r\n",
        "    tryCatch({\r\n",
        "      if(is_legacy_data_available & separate_data){\r\n",
        "        verified_data_df <- separate_control_dataframe(verified_data_df, legacy_df, has_authorative_ID)\r\n",
        "      }\r\n",
        "    }, error = function(e) {\r\n",
        "      print(paste(\"Error seperating control data. All data has been treated as new entries.\", conditionMessage(e)))\r\n",
        "    })\r\n",
        "\r\n",
        "\r\n",
        "    ### Save & Aggregate Data -------------------------------------------------------------\r\n",
        "    \r\n",
        "    # Save unaggrgated workflow with specific naming convension output\r\n",
        "    tryCatch({\r\n",
        "      if (!dir.exists(configuration$metadata$output_directory$control_data_unaggregated)) {\r\n",
        "        dir.create(configuration$metadata$output_directory$control_data_unaggregated, recursive = TRUE)\r\n",
        "      }\r\n",
        "      \r\n",
        "      output_directory <- configuration$metadata$output_directory$control_data_unaggregated\r\n",
        "      data_type <- configuration$metadata$control_data_type\r\n",
        "      timestamp <- format(Sys.time(), \"%Y%m%d_%H%M%S\")\r\n",
        "      file_name <- paste(data_type, \"_\", timestamp, \".csv\", sep = \"\")\r\n",
        "      output_path <- file.path(output_directory, file_name)\r\n",
        "      write.csv(verified_data_df, output_path, row.names = FALSE)\r\n",
        "      \r\n",
        "    }, error = function(e) {\r\n",
        "      print(paste(\"Error saving data - Data saved in source directory\", conditionMessage(e)))\r\n",
        "      write.csv(verified_data_df, paste(configuration$metadata$control_data_type,\"_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".csv\", sep = \"\"), row.names = FALSE)\r\n",
        "  \r\n",
        "    })\r\n",
        "\r\n",
        "    # Aggregate the cull an manta tow data \r\n",
        "    if(configuration$metadata$control_data_type == \"manta_tow\"){\r\n",
        "      verified_aggregated_df <- aggregate_manta_tows_site_resolution_research(verified_data_df)  \r\n",
        "    } else if (configuration$metadata$control_data_type == \"cull\") {\r\n",
        "      verified_aggregated_df <- aggregate_culls_site_resolution_research(verified_data_df) \r\n",
        "    }\r\n",
        "\r\n",
        "    # Save aggregated data\r\n",
        "    tryCatch({\r\n",
        "      if (!dir.exists(configuration$metadata$output_directory$control_data_aggregated)) {\r\n",
        "        dir.create(configuration$metadata$output_directory$control_data_aggregated, recursive = TRUE)\r\n",
        "      }\r\n",
        "      \r\n",
        "      output_directory <- configuration$metadata$output_directory$control_data_aggregated\r\n",
        "      data_type <- configuration$metadata$control_data_type\r\n",
        "      timestamp <- format(Sys.time(), \"%Y%m%d_%H%M%S\")\r\n",
        "      file_name <- paste(data_type, \"_\", timestamp, \".csv\", sep = \"\")\r\n",
        "      output_path <- file.path(output_directory, file_name)\r\n",
        "      write.csv(verified_aggregated_df, output_path, row.names = FALSE)\r\n",
        "      \r\n",
        "    }, error = function(e) {\r\n",
        "      print(paste(\"Error saving data - Data saved in source directory\", conditionMessage(e)))\r\n",
        "      write.csv(verified_aggregated_df, paste(configuration$metadata$control_data_type,\"_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".csv\", sep = \"\"), row.names = FALSE)\r\n",
        "      \r\n",
        "    })\r\n",
        "    \r\n",
        "    # Reset\r\n",
        "    new_path <- NULL\r\n",
        "    configuration_path <- NULL\r\n",
        "    kml_path <- NULL\r\n",
        "    leg_path <- NULL\r\n",
        "    \r\n",
        "  }, error = function(e) {\r\n",
        "    print(paste(\"Critical Error in workflow could not be resolved:\", conditionMessage(e)))\r\n",
        "  })\r\n",
        "}\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute Workflow"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "configuration_path <- NULL\r\n",
        "kml_path <- NULL\r\n",
        "leg_path <- NULL\r\n",
        "\r\n",
        "main(new_path, configuration_path = NULL, kml_path = NULL, leg_path = NULL)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "r"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}